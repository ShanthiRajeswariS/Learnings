{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanisum"
      ],
      "metadata": {
        "id": "vzg01ghvwsW6"
      },
      "id": "vzg01ghvwsW6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Numerous tasks in natural language processing (NLP) depend heavily on an attention mechanism. When the data is being processed, they allow the model to **focus on only certain input elements, such as specific words or phrases**.\n",
        "\n",
        "- This is important for NLP tasks because the input data, often given as sentences or paragraphs, can be long and complicated, making it hard for the model to figure out the most critical information.\n",
        "\n",
        "- A model computes **attention weights for each position in the input after first representing the input as a collection of query, key, and value vectors**. This is how attention mechanisms operate. These attention weights indicate the **importance of each position in the input** for the task at hand. The **output** of the attention mechanism for the current position is then represented by a **weighted sum that is computed using the attention weights to weigh the input**."
      ],
      "metadata": {
        "id": "yh5SVTKYwsaC"
      },
      "id": "yh5SVTKYwsaC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Numerous NLP tasks, such as **machine translation, language modelling, text summarisation, and question answering**, have used attention mechanisms.\n",
        "- They have been particularly effective in tasks like machine translation, where the model must comprehend the **relationships between words in various languages**.\n",
        "- Additionally, they have been incorporated into **transformers and their variants**."
      ],
      "metadata": {
        "id": "qGr4MUqMwsc5"
      },
      "id": "qGr4MUqMwsc5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the attention mechanism?"
      ],
      "metadata": {
        "id": "DMgXmdIxwsfX"
      },
      "id": "DMgXmdIxwsfX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The attention mechanism allows neural networks **to concentrate on particular input components/ to focus on the most relevant parts of the input sequence when producing an output**.\n",
        "- It enables the model to selectively focus on the input portions that are most pertinent to the task at hand and to weigh the relative importance of various input components.\n",
        "- Attention mechanisms have been applied in multiple natural language processing tasks, including **language modelling and machine translation**, as well as in other fields like **computer vision**.\n",
        "\n",
        "**The attention mechanism allows neural networks to concentrate on particular input components**\n",
        "\n",
        "\n",
        "- Instead of trying to encode the entire input into a single fixed-size representation (a common limitation of earlier Recurrent Neural Networks, or RNNs), attention allows the model to selectively \"pay attention\" to different parts of the input at each step of the output generation. This is analogous to how humans focus on specific details or words when reading or translating.\n",
        "\n",
        "- The \"importance\" is typically represented by \"soft\" weights assigned to each element (e.g., word, token) in a sequence. Higher weights indicate greater relevance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5vpm8uM8wsiJ"
      },
      "id": "5vpm8uM8wsiJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Need of Attention Mechanisum"
      ],
      "metadata": {
        "id": "XGVI44RO58H8"
      },
      "id": "XGVI44RO58H8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before attention, **RNNs and LSTMs** were prevalent for sequential data. However, they faced significant challenges:\n",
        "\n",
        "- **Long-range dependencies**: As sequences grew longer, RNNs struggled to retain information from earlier parts of the input, leading to a \"bottleneck\" effect where the final hidden state couldn't adequately summarize the entire context. This made it difficult to capture relationships between words that were far apart in a sentence.\n",
        "\n",
        "- **Sequential processing**: RNNs process data step-by-step, which is inherently slow and prevents parallel computation, making them inefficient for very long sequences and large datasets.\n",
        "\n",
        "- **Fixed-size context vector**: In traditional encoder-decoder models for tasks like machine translation, the encoder compressed the entire input sequence into a single fixed-length \"context vector.\" This vector often failed to capture all the nuances of long sentences.\n",
        "\n",
        "The attention mechanism was introduced to mitigate these issues by providing a **flexible way for the decoder to access all parts of the input sequence at each decoding step, rather than relying solely on the final hidden state of the encoder.**"
      ],
      "metadata": {
        "id": "P60bpvdf6AUj"
      },
      "id": "P60bpvdf6AUj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Attention Mechanisum works?"
      ],
      "metadata": {
        "id": "h5zw5gDV6hbU"
      },
      "id": "h5zw5gDV6hbU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention mechanism typically involves three key components:\n",
        "\n",
        "1.  **Query (Q):** Represents the current element or state for which we want to compute a **contextual representation**. Think of it as \"What am I looking for?\"\n",
        "\n",
        "2.  **Key (K):** **Represents all elements in the input sequence against which the Query is compared**. Think of them as \"What are the available pieces of information?\"\n",
        "\n",
        "3.  **Value (V):** These are the **actual representations of the input elements that will be weighted and summed**. Think of them as \"What information do these pieces hold?\" (Often, Keys and Values are derived from the same source sequence, where the **Key** is used for **computing similarity** and the **Value** is the **actual information being passed through**).\n"
      ],
      "metadata": {
        "id": "OyoRDKVm6ktD"
      },
      "id": "OyoRDKVm6ktD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The core steps are:\n",
        "\n",
        "* **Similarity Calculation:** An attention score is calculated by comparing the Query with each Key in the input sequence. Common similarity functions include **dot product, scaled dot product (used in Transformers), or additive (concatenation-based) attention.**\n",
        "* **Softmax Normalization:** These scores are then passed through a softmax function **to obtain attention weights**. These weights are normalized, **positive values that sum to 1, representing the probability distribution of \"attention\" over the input elements.**\n",
        "* **Weighted Sum:** The attention weights are then used to compute a weighted sum of the Value vectors. This **weighted sum becomes the \"context vector\" or the attended output**, effectively focusing on the most relevant information from the input based on the Query.\n",
        "\n",
        "**Mathematical Representation (Scaled Dot-Product Attention):**\n",
        "The most common form, especially in Transformers, is Scaled Dot-Product Attention:\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "Where:\n",
        "* $Q$ is the Query matrix.\n",
        "* $K$ is the Key matrix.\n",
        "* $V$ is the Value matrix.\n",
        "* $d_k$ is the dimension of the Key vectors, used for scaling to prevent large dot product values from pushing the softmax into regions with tiny gradients.\n",
        "* $K^T$ is the transpose of the Key matrix."
      ],
      "metadata": {
        "id": "_gV526cz7IH8"
      },
      "id": "_gV526cz7IH8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention mechanism in NLP"
      ],
      "metadata": {
        "id": "2cnNRkjSwsk6"
      },
      "id": "2cnNRkjSwsk6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The attention mechanism in a machine translation model enables the model to **concentrate on particular elements of the source sentence when decoding the target sentence**.\n",
        "\n",
        "- Attention mechanisms let the model pay attention to certain input parts and learn how vital different elements are.\n",
        "\n",
        "\n",
        "- Example: “The cat sat on the mat.”\n",
        "\n",
        "  - When the model is translating the word “gato” (the Spanish word for “cat”), it needs to know which noun in the English sentence it refers to. The attention mechanism allows the model to focus on the word “cat” in the English sentence and use it as the reference to translate “gato”.\n",
        "\n",
        "  - The attention mechanism works by **first encoding the source sentence into a set of hidden states and then computing a set of attention weights for each word in the target sentence to show how vital each hidden state was in producing the current word**. The **hidden states are then weighed using the attention weights to create a weighted sum used as the input to the current word’s decoder**.\n",
        "\n",
        "- In this example, the model will have high weights for the hidden states associated with the word “cat” to decode “gato,” enabling the model to concentrate on that particular section of the source sentence.\n",
        "\n"
      ],
      "metadata": {
        "id": "TriAHd96wsnS"
      },
      "id": "TriAHd96wsnS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-attention/ Intra-attention"
      ],
      "metadata": {
        "id": "PSKKYxitykyH"
      },
      "id": "PSKKYxitykyH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Self-attention is a deep learning mechanism that lets a model focus on different parts of an input sequence by giving each part a weight to figure out how important it is for making a prediction\n",
        "\n",
        "- Type of attention mechanism in which the **model attends to different positions of the input sequence by comparing each position with all other positions.**\n",
        "\n",
        "- Self-attention allows the model to weigh the importance of various elements in the input sequence and selectively focus on the parts of the input that are most relevant to the task at hand.\n",
        "\n",
        "- It’s primarily used in models like a transformer and their variants. In a **transformer**, the self-attention mechanism is used **to process the input sequence, allowing the model to learn relationships between the different elements of the input without explicitly being told which parts are related**.\n",
        "\n",
        "- It lets a model decide **how important each part of an input sequence is, which makes it possible to find dependencies and connections in the data.**\n",
        "\n",
        "- In self-attention, a model **calculates the attention weights between each element in the input sequence, allowing it to focus on the relevant factors for a given task**. This mechanism works very well because it lets the model take into account long-term dependencies and relationships in the data, which improves performance on many jobs.\n",
        "\n",
        "- The model uses this self-attention mechanism to decide which parts of the input to focus on dynamically. In addition, it allows it to handle input sequences of varying lengths and capture dependencies between elements in the series."
      ],
      "metadata": {
        "id": "0wmW2MPhyl_u"
      },
      "id": "0wmW2MPhyl_u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention vs self-attention"
      ],
      "metadata": {
        "id": "VYV-lDcmymCm"
      },
      "id": "VYV-lDcmymCm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "9S_hmz5vymFm"
      },
      "id": "9S_hmz5vymFm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- When a model processes input, the term “attention” refers to letting it **concentrate on particular portions of the input**.\n",
        "\n",
        "- It enables the model **to selectively focus on the input portions that are most pertinent to the task at hand and to weigh the relative importance of various input components**.\n",
        "\n",
        "- Numerous tasks, including **computer vision, language modelling, and machine translation**, have used attention mechanisms.\n",
        "\n",
        "- Attention refers to a mechanism in which a model calculates attention scores between different parts of an input and another part of the input or external memory. For example, in machine translation, the attention mechanism calculates attention scores between the source sentence and the target sentence, allowing the model to weigh the importance of each part of the source sentence in the target translation.\n",
        "\n"
      ],
      "metadata": {
        "id": "SduYv-MFzjfG"
      },
      "id": "SduYv-MFzjfG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention"
      ],
      "metadata": {
        "id": "Kyk8ynofz32m"
      },
      "id": "Kyk8ynofz32m"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- By comparing **each position to all the different positions, the model pays attention to various parts in the input sequence using this attention mechanism**.\n",
        "\n",
        "- Self-attention enables the model **to selectively focus on the input portions that are most pertinent to the task at hand and weigh the relative importance of various components in the *input sequence*.**\n",
        "\n",
        "- Most often, it is utilised in models like **transformers and their variations**\n",
        "\n",
        "- self-attention is a mechanism by which the model calculates attention scores between different parts of the input sequence without using external memory. Self-attention lets the model figure out how important each part of the series is, determine how the parts depend on each other and make predictions based on that."
      ],
      "metadata": {
        "id": "uhQlvLY7z5_1"
      },
      "id": "uhQlvLY7z5_1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short, attention is a mechanism in which a model calculates attention scores between different parts of an input and another part of the input or external memory. On the other hand, self-attention is a mechanism in which the model only calculates attention scores between different parts of the input sequence."
      ],
      "metadata": {
        "id": "HyHWqQO680so"
      },
      "id": "HyHWqQO680so"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Types of Attention Mechanisum in NLP"
      ],
      "metadata": {
        "id": "wCaOunuUz6C1"
      },
      "id": "wCaOunuUz6C1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self- Attention/ Intra-Attention"
      ],
      "metadata": {
        "id": "Nx9MLcj1RFfO"
      },
      "id": "Nx9MLcj1RFfO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This is a crucial variant where the **Query, Key, and Value** all come from the *same* input sequence.\n",
        "- This allows the model to understand the **relationships between different words within a single sentence, capturing dependencies regardless of their distance.**\n",
        "- For example, in \"The animal didn't cross the street because **it** was too tired,\" self-attention helps the model understand that \"it\" refers to \"animal.\"\n",
        "- Self-attention is fundamental to the Transformer architecture."
      ],
      "metadata": {
        "id": "CeYNd8ghRIKe"
      },
      "id": "CeYNd8ghRIKe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of Self Attention"
      ],
      "metadata": {
        "id": "lGb0tUgv7hZN"
      },
      "id": "lGb0tUgv7hZN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Dot-product attention**: The attention scores are calculated as the dot product of the queries and keys. This type of self-attention is used in the Transformer architecture.\n",
        "\n",
        "- **Scaled dot-product attention** is similar to dot-product attention, but the attention scores are divided by the square root of the number of dimensions of the queries and keys to ensure they are stable.\n",
        "\n",
        "- **Multi-head attention**: Multiple attention heads capture different aspects of the input sequence. Each head calculates its own set of attention scores, and the results are concatenated and transformed to produce the final attention weights.\n",
        "\n",
        "- **Local attention**: The attention mechanism is only used on several elements in the input sequence. This lets the model focus on dependencies close to the sequence’s beginning or end.\n",
        "\n",
        "- **Additive attention**: The attention scores are calculated as a function of the similarity between the queries and keys rather than just their dot product.\n",
        "\n",
        "- **Cosine attention** scores are calculated as the cosine similarity between the queries and keys."
      ],
      "metadata": {
        "id": "VSCG19tN7kbd"
      },
      "id": "VSCG19tN7kbd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Attention/ Encoder-Decoder Attention"
      ],
      "metadata": {
        "id": "1O2wOlX1RzgG"
      },
      "id": "1O2wOlX1RzgG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Used primarily in the **decoder of a Transformer model** (or other sequence-to-sequence models).\n",
        "- Here, the Query comes from the *decoder's current state*, while the Key and Value come from the *encoder's output sequence*.\n",
        "- This allows the decoder to attend to the most relevant parts of the source sequence when generating each part of the target sequence (e.g., in machine translation, deciding which source words to focus on when generating a specific target word)."
      ],
      "metadata": {
        "id": "5BYhjTCuR1HV"
      },
      "id": "5BYhjTCuR1HV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaled dot-product attention"
      ],
      "metadata": {
        "id": "RoZG4d1l0oFW"
      },
      "id": "RoZG4d1l0oFW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To calculate the **input sequence’s attention weights for each position**. It is the most commonly used type of attention in **transformer architecture**.\n",
        "\n",
        "- The attention mechanism first represents the **input sequence as queries, keys, and value matrices**.\n",
        "\n",
        "- The **query** matrix shows the **current position** in the input sequence, the **key** matrix shows **all the other positions in the input sequence**, and the **value** matrix shows the **information that should be output for each position in the input sequence**.\n",
        "\n",
        "- The **attention weights** for each position are calculated by taking the **dot product of the query matrix and the key matrix and then dividing by the square root of the dimension of the key matrix**. This is done so that the dot product stays manageable and makes the numbers stable.\n",
        "\n",
        "- The attention weights are then used **to weigh the value matrix and figure out the output of the attention mechanism for the current position, which is a weighted sum.**\n",
        "\n",
        "- In **transformer** architecture and its variations, the scaled dot-product attention is used for the **model to pay attention to different points in the input sequence by comparing them to all the other points**. The **attention weights are then used to weigh the hidden states and compute a weighted sum, which is used as the input to the decoder for the current word**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DGgxXIEI0oIW"
      },
      "id": "DGgxXIEI0oIW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention"
      ],
      "metadata": {
        "id": "B0CufgwO0oLE"
      },
      "id": "B0CufgwO0oLE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Multi-head attention is a **variant of scaled dot-product attention** where **multiple attention heads are used in parallel**. Each attention head **learns to attend to a different representation of the input**.\n",
        "\n",
        "- In multi-head attention, the input is first transformed into **multiple different representations (also called heads) using linear transformations**. These representations are then used to compute **numerous sets of attention weights, one for each head**. The **attention weights are then used to weigh the input and calculate a weighted sum, which is the output of the attention mechanism for the current position.**\n",
        "\n",
        "- The idea behind using multiple attention heads is that **each head can focus on a different aspect of the input**. By using multiple heads, the model can **learn to attend to different types of information in the input,** and the final output is a **concatenation of all the weighted sum from each head**.\n",
        "\n",
        "- The multi-head attention mechanism is used in transformer architecture and its variants t**o improve the model’s ability to attend to different parts of the input and learn multiple representations of the input.**\n",
        "\n",
        "- An extension where the attention mechanism is performed multiple times in parallel, using different learned linear projections (different \"heads\") for the Query, Key, and Value matrices.\n",
        "- Each head can learn to focus on different aspects or relationships in the input (e.g., syntactic, semantic), and their outputs are concatenated and linearly transformed to produce the final result. This enriches the model's understanding.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JFl1Fqxx0oN2"
      },
      "id": "JFl1Fqxx0oN2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additive attention"
      ],
      "metadata": {
        "id": "wGPd07Au0oRt"
      },
      "id": "wGPd07Au0oRt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Additive attention is a type of attention mechanism similar to scaled dot-product attention, but the attention weight is calculated differently. Instead of taking the dot product of the query and key matrices and scaling the result, **additive attention computes the similarity measure between the query and key vectors**. Then it applies a **feed-forward neural network to produce attention weights**.\n",
        "\n",
        "- The process starts by representing the **input sequence as a query, key, and value vector**. Then the **attention weights** are computed **by taking the dot product of the query vector with the key vector and passing it through a feed-forward neural network, which produces a scalar value that is used as the attention weight.** This attention weight is then used **to weigh the value vector and compute a weighted sum, which represents the output of the attention mechanism for the current position**.\n",
        "\n",
        "- One of the **main advantages** of additive attention is that it **allows the model to learn a non-linear similarity measure between the query and key vectors**. This can be useful in cases where the dot product similarity measure is inappropriate.\n",
        "\n",
        "- Additive attention is less widely used than scaled dot-product and multi-head attention. Still, it has been used in some architectures and tasks, especially in some **conversational models**, and it has shown to be helpful in specific scenarios.\n",
        "\n"
      ],
      "metadata": {
        "id": "rqWBFmL9z6F1"
      },
      "id": "rqWBFmL9z6F1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Location-based attention"
      ],
      "metadata": {
        "id": "607ykH2kz6Im"
      },
      "id": "607ykH2kz6Im"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Location-based attention is a type of attention mechanism that allows the model **to focus on specific input regions using a convolutional neural network (CNN) to learn a set of attention weights**.\n",
        "\n",
        "- In location-based attention, the **input is first passed through a CNN to produce a set of feature maps**.\n",
        "\n",
        "- These feature maps represent the **input at different positions and scales**. Then, the **feature maps compute a set of attention weights for each position**. Finally, the **attention weights are calculated by applying a 1×1 convolution on the feature maps, which produces a scalar value for each position.**\n",
        "\n",
        "- These scalar values are then used as **attention weights**. The attention weights are then **used to weigh the feature maps and compute a weighted sum, which represents the output of the attention mechanism for the current position**.\n",
        "\n",
        "- The **main advantage** of location-based attention is that it allows the model **to focus on specific regions of the input rather than just individual positions in the input sequence**. This can be useful in tasks such as **image captioning**, where the model needs to focus on specific regions of an image when generating a caption.\n",
        "\n",
        "- Location-based attention has been used in some architectures and tasks, particularly in **computer vision**, and it has proven helpful in some scenarios.\n",
        "\n"
      ],
      "metadata": {
        "id": "RQQWiVu229UV"
      },
      "id": "RQQWiVu229UV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Co-Attention"
      ],
      "metadata": {
        "id": "Dc5lX_Cn3e-G"
      },
      "id": "Dc5lX_Cn3e-G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Co-attention is a type of attention mechanism used **when there are multiple inputs, and it allows the model to learn the relationship between the different inputs**. It’s primarily used in **visual question answering, image, and video captioning**. The model needs to understand the relationship between the various inputs to decide.\n",
        "\n",
        "- In co-attention, the **model simultaneously attends to both the visual and textual input**, and the **attention weights are computed for each input independently**. These **attention weights are then used to weigh the input and calculate a weighted sum, representing the output of the attention mechanism for the current position**.\n",
        "\n",
        "- The model uses the attention weights **to identify essential parts of the visual input and then uses that information to understand the text input**.\n",
        "\n",
        "- Co-attention can be divided into two types:\n",
        "  - 1.**Hard co-attention** - Used when the model **makes a hard decision about which input is essential for each position**\n",
        "  - 2.**Soft co-attention** - Used when the model makes a soft decision.\n",
        "\n",
        "- Co-attention mechanisms are helpful in tasks **where the model needs to understand the relationship between multiple inputs**. It’s been used in some architectures and functions, especially in **computer vision and natural language processing**, and it has shown to be helpful in specific scenarios.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tgo_KBVY3fAk"
      },
      "id": "Tgo_KBVY3fAk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention with external memory"
      ],
      "metadata": {
        "id": "aZx74AIK3fFs"
      },
      "id": "aZx74AIK3fFs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Attention with external memory refers to a type of attention mechanism that allows the model **to incorporate an external memory or knowledge base to improve decision-making.**\n",
        "\n",
        "- In this type of attention, the model uses **an external memory bank to store and retrieve information that is relevant to the task at hand**. The model uses **attention weights to focus on specific parts of the external memory, similar to how it focuses on particular aspects of the input**.\n",
        "\n",
        "- There are several ways to implement attention with external memory. Still, one common approach is to use a **memory-augmented neural network**, such as the **Neural Turing Machine (NTM) or the Differentiable Neural Computer (DNC)**. In addition to a neural network, these architectures also use an outside memory bank to do the job.\n",
        "\n",
        "- The external memory bank can store information essential to the task, like previous inputs, and the attention mechanism is used to get the data from the memory bank.\n",
        "\n",
        "- Attention with external memory has been used in many tasks, such as **language modelling, machine translation, and question answering**. It is helpful when the model needs external knowledge to make a decision."
      ],
      "metadata": {
        "id": "frYnm1-14T4t"
      },
      "id": "frYnm1-14T4t"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "eFthy8j94_ds"
      },
      "id": "eFthy8j94_ds"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Scaled dot-product attention**: This is the most common type of self-attention, and it is used in transformer architecture. It takes the dot product of the query, key, and value matrices and scaling the dot product by the square root of the dimension of the key matrix.\n",
        "\n",
        "2. **Multi-head attention**: This is a variant of scaled dot-product attention where multiple attention heads are used in parallel. Each attention head learns to attend to a different representation of the input.\n",
        "\n",
        "3. **Additive attention**: This attention mechanism is similar to scaled dot-product attention, but the attention weight is calculated differently. It computes the similarity measure between the query and key vectors and then applies a feed-forward neural network to produce attention weights.\n",
        "\n",
        "4. **Location-based attention**: this type of attention allows the model to focus on specific input regions using a convolutional neural network to learn a set of attention weights.\n",
        "\n",
        "5. **Co-Attention**: This attention mechanism is used when there are multiple inputs, allowing the model to learn the relationship between the different inputs.\n",
        "\n",
        "6. **Attention with external memory**: This attention mechanism allows the model to incorporate an external memory or knowledge base to improve decision-making."
      ],
      "metadata": {
        "id": "7ZpLNLgX5AXr"
      },
      "id": "7ZpLNLgX5AXr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The attention mechanism, particularly self-attention and multi-head attention, is the cornerstone of the **Transformer architecture**, which has become the dominant model for most state-of-the-art NLP tasks. Models like BERT, GPT-x series, T5, and many others are built upon the Transformer and its attention mechanisms.\n",
        "\n",
        "In summary, the attention mechanism is a powerful technique that has transformed NLP by enabling models to dynamically focus on relevant information within sequences, leading to significant improvements in performance, scalability, and even interpretability."
      ],
      "metadata": {
        "id": "nYXQGIanUUV0"
      },
      "id": "nYXQGIanUUV0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benefits of Attention Mechanism in NLP"
      ],
      "metadata": {
        "id": "soJ_IyTqT7d9"
      },
      "id": "soJ_IyTqT7d9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  **Handles Long-Range Dependencies:** It directly **addresses the vanishing gradient problem and the fixed-size context vector bottleneck of RNNs** by allowing direct access to all parts of the input, regardless of their position.\n",
        "\n",
        "2.  **Improved Performance:** Models equipped with attention mechanisms consistently achieve state-of-the-art results across a wide range of NLP tasks, including **machine translation, text summarization, question answering, and language modeling**.\n",
        "\n",
        "3.  **Parallelization:** Especially with self-attention, the computations for each word's attention can be performed simultaneously, enabling parallel processing on hardware like GPUs. This significantly speeds up training compared to sequential RNNs.\n",
        "\n",
        "4.  **Interpretability:** The attention weights provide a degree of interpretability. By visualizing these weights, one can see which input words or phrases the model focused on when generating a particular output. This offers insights into the model's decision-making process.\n",
        "\n",
        "5.  **Reduced Inductive Bias:** Unlike RNNs that have an inductive bias towards sequential processing, attention mechanisms are more flexible and can learn complex dependencies without being constrained by sequence o rder, as long as positional information (like positional encodings in Transformers) is provided."
      ],
      "metadata": {
        "id": "14yJZDpmT8bU"
      },
      "id": "14yJZDpmT8bU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "4uVchFHJ7GuP"
      },
      "id": "4uVchFHJ7GuP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The Transformer is an architecture for deep learning that uses mechanisms for self-attention to process sequential data like text.\n",
        "\n",
        "- In the Transformer, self-attention is used **to determine how much attention each part of the input sequence gets**. This lets the model **figure out each part’s importance and make predictions based on that.**\n",
        "\n",
        "- The attention mechanism allows the Transformer to **capture long-range dependencies in the input sequence and handle input of varying lengths. **\n",
        "\n",
        "- The Transformer has become one of the most popular architectures in natural language processing and has done state-of-the-art work on various tasks."
      ],
      "metadata": {
        "id": "YjDQwoIq7FBk"
      },
      "id": "YjDQwoIq7FBk"
    },
    {
      "cell_type": "markdown",
      "id": "a765fd20",
      "metadata": {
        "id": "a765fd20"
      },
      "source": [
        "# Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68780d9",
      "metadata": {
        "id": "d68780d9"
      },
      "source": [
        "- https://spotintelligence.com/2023/01/12/attention-mechanism-in-nlp/\n",
        "\n",
        "- https://spotintelligence.com/2023/01/31/self-attention/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}