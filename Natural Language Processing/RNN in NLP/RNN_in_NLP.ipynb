{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Best RNN for NLP"
      ],
      "metadata": {
        "id": "OBV5IrCoqN4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Elman RNNs\n",
        "- Long short-term memory (LSTM) networks\n",
        "- Gated recurrent units (GRUs)\n",
        "- Bi-directional RNNs and Transformer networks"
      ],
      "metadata": {
        "id": "NvszHxYWqN7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN"
      ],
      "metadata": {
        "id": "McFiFicPqN9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A recurrent neural network (RNN) is an artificial neural network that **works well with data that comes in a certain order**.\n",
        "\n",
        "- RNNs are useful for tasks like **translating languages, recognising speech, and adding captions to images**.\n",
        "- This is because they can process **sequences of inputs and turn them into sequences of outputs**.\n",
        "- One thing that makes RNNs different is that they **have “memory.”** This lets them keep data **from previous inputs in the current processing step**.\n",
        "- To do this, **hidden states are used**. They are **changed at each time step as the input sequence is processed and stored in memory**.\n",
        "- RNNs can **unroll a sequence of inputs over time to show how they dealt with them step by step.**\n",
        "\n"
      ],
      "metadata": {
        "id": "Aq1RiuVptGyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN in NLP\n"
      ],
      "metadata": {
        "id": "88652XxptG2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Natural language processing (NLP) tasks like **language translation, speech recognition, and text generation frequently** use **recurrent neural networks**(RNNs).\n",
        "\n",
        "- They can handle **input sequences of different lengths and produce output sequences of various sizes**. This makes them great for NLP tasks.\n",
        "\n",
        "- In NLP, RNNs are frequently used in machine translation **to process a sequence of words in one language and generate a corresponding series of words in a different language as the output**.\n",
        "\n",
        "- **Language modelling**, which involves predicting the **following word in a sequence based on the preceding terms**, is another application for RNNs. This can be used, for instance, to create text that appears to have been written by a person.\n",
        "\n",
        "- One thing that makes RNNs different is that they have **“memory”**.\n",
        "\n",
        "- RNNs can also classify text by determining whether a **passage is positive or negative.** Or identifying **named entities, such as people, organisations, and places mentioned in a passage**.\n",
        "\n",
        "- RNNs can capture the **relationships between words in a sequence** and use this knowledge **to predict the next word in the series**. This makes them an effective tool for NLP tasks in general.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JIohjxbftG55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Types of RNN used in NLP"
      ],
      "metadata": {
        "id": "sLat-0NKqOAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elman RNNs (Hidden State as Memory)"
      ],
      "metadata": {
        "id": "zkxANM4jqOC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- one of the most basic types of RNNs and is often used as a foundation for more complex RNN architectures.\n",
        "\n",
        "- An Elman RNN processes **the input sequence one element at a time and has a single hidden layer**.\n",
        "- The **current input element and the previous hidden state** are inputs the hidden layer uses to produce an output and update the **hidden state at each time step**. As a result, the Elman RNN can **retain data from earlier input and use it to process the input at hand**.\n",
        "\n",
        "- Elman RNNs are frequently employed for **processing sequential data, such as speech and language translation**.\n",
        "- They are easier to build and train than more complicated RNN architectures like long short-term memory (LSTM) networks and gated recurrent units (GRUs). However, they may not perform as well.\n",
        "\n"
      ],
      "metadata": {
        "id": "o1kzzDUVqOII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long short-term memory (LSTM) networks - (Memory cell and Gates)"
      ],
      "metadata": {
        "id": "2xxd3lZ5qOKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Recurrent neural networks (RNNs) of the type known as **long short-term memory (LSTM)** networks can recognise **long-term dependencies in sequential data**.\n",
        "\n",
        "- They are beneficial in **language translation, speech recognition, and image captioning**.\n",
        "- The input sequence can be **very long**, and the **elements’ dependencies can extend over numerous time steps.**\n",
        "\n",
        "- “**Memory cells,”** which can **store data for a long time**, and **“gates,** which **regulate the information flow into and out of the memory cells**, make up LSTM networks.\n",
        "- LSTMs are especially good **at finding long-term dependencies because they can choose what to remember and what to forget.**\n",
        "\n",
        "- Elman RNNs and gated recurrent units (GRUs) are two examples of other RNNs that are typically simpler and easier to train than LSTM networks.\n",
        "- However, LSTM networks are generally more powerful and perform better across various tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "wzJGzC6LvaxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gated recurrent units (GRUs) - Reset Gate and Update Gate"
      ],
      "metadata": {
        "id": "9oOhBmZ7va0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Long short-term memory (LSTM) networks and gated recurrent units (GRUs) are two types of recurrent neural networks (RNNs), but **GRUs have fewer parameters and are typically simpler to train.**\n",
        "\n",
        "- Like LSTMs, GRUs are effective for **speech recognition, image captioning, and language translation because they can identify long-term dependencies in sequential data.**\n",
        "\n",
        "- **Update gates and reset gates** are the two different types of gates found in GRUs.\n",
        "- The **reset gate** decides **what information should be forgotten**, and the **update gate** decides **what information should be kept from the previous time step**.\n",
        "- As with LSTMs, this enables GRUs to remember or omit information selectively.\n",
        "\n",
        "- GRUs are an excellent option for many NLP tasks, even though they are typically less effective than LSTMs due to their simplicity and ease of training.\n",
        "- Also, they use less energy to run, which can be crucial in places where resources are scarce.\n",
        "\n"
      ],
      "metadata": {
        "id": "3LYj0A4ava3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bi-directional RNNs"
      ],
      "metadata": {
        "id": "D0nGlPN-va5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- An RNN that **processes the input sequence forward and backwards**, allowing the model **to capture dependencies in both directions**, is known as a bi-directional recurrent neural network (RNN).\n",
        "\n",
        "- This is helpful for tasks like **language translation and language modelling**, where the context of a word can depend on both past and future words.\n",
        "\n",
        "- One RNN processes the input sequence in the forward direction, and the other RNN processes the series in the backward direction, making up a bi-directional RNN.\n",
        "- At e**ach time step**, the forward and backward RNNs’ outputs are added together, and the resulting sequence is the final output of the model.\n",
        "\n",
        "- Bi-directional RNNs are more complex and potentially more challenging to train than uni-directional RNNs, which only process the input sequence in one direction.\n",
        "- However, they are typically more powerful. Therefore, they are generally employed when a word’s context depends on previous and upcoming words.\n",
        "\n"
      ],
      "metadata": {
        "id": "nKN47rx9va8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer networks"
      ],
      "metadata": {
        "id": "DYgpUpN8w1EJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Transformer neural networks **process sequential data using self-attention** instead of recurrence, as in conventional recurrent neural networks (RNNs).\n",
        "- They have recently become more popular for natural language processing (NLP) tasks and have beaten many benchmarks with the best results available today.\n",
        "\n",
        "- Transformer networks are a **stack of self-attention layers for both the encoder and the decoder**.\n",
        "\n",
        "- First, the **encoder** processes the **input sequence**, which creates a **fixed-length representation** that is then given to the **decoder**. Next, the decoder uses **this representation to produce the output sequence**.\n",
        "\n",
        "- Using self-attention, transformers can efficiently **process very long sequences by recognising long-term dependencies in the input sequence**.\n",
        "\n",
        "- As a result, they are a good option for tasks like **machine translation and language modelling** because they are also very efficient to train and are **simple to parallelise**.\n",
        "\n",
        "- There is no single “best” type of RNN for all NLP tasks. The best type will depend on the particular task and the resources available (such as computational power and data)."
      ],
      "metadata": {
        "id": "fymcAYMww1G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of RNN in NLP"
      ],
      "metadata": {
        "id": "FrKCIFuqw1Jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Before using the data, you need to **tokenise** the text; this can be done with **stemming, lemmatization, word embeddings or sentence embeddings**.\n",
        "\n",
        "2.**Build the model**: This entails specifying the RNN’s architecture, including its **layer count, the size of its hidden states, and the recurrent unit type (such as an LSTM or GRU)**.\n",
        "\n",
        "3.**Train the model**: To minimise a loss function, such as cross-entropy loss, **the model’s parameters must be optimised** while being fed the preprocessed data.\n",
        "\n",
        "4.**Evaluate the model**: This entails evaluating the model’s performance on a held-out test set using metrics like accuracy or perplexity.\n",
        "\n",
        "5.**Use the model**: The model can carry out the desired NLP task, such as text generation or language translation, after being trained and evaluated."
      ],
      "metadata": {
        "id": "LVwc_qn0w1MZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gzVOCuGXw1Op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source"
      ],
      "metadata": {
        "id": "Bsu2kpo0qONR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://spotintelligence.com/2023/01/07/rnn-in-nlp/"
      ],
      "metadata": {
        "id": "l1zYV378s2pK"
      }
    }
  ]
}