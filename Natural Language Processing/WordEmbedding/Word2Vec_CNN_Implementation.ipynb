{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBdM5peVWpWw",
        "outputId": "3a8d9333-870a-4f32-d5e5-dc5a238384cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n"
      ],
      "metadata": {
        "id": "qRpf3XJVWGAP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Data (replace with your actual data)\n",
        "texts = [\n",
        "    \"This is a positive review about the excellent product.\",\n",
        "    \"The movie was absolutely terrible and boring.\",\n",
        "    \"I really enjoyed the fantastic service.\",\n",
        "    \"This is the worst experience I've ever had.\",\n",
        "    \"The food was delicious and the staff was friendly.\",\n",
        "    \"A completely waste of time and money.\",\n",
        "]\n",
        "labels = np.array([1, 0, 1, 0, 1, 0])  # 1 for positive, 0 for negative\n"
      ],
      "metadata": {
        "id": "-CT2z64wWGDA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization and Vocabulary Building\n",
        "tokenizer = Tokenizer(num_words=100)  # Consider a larger vocabulary size\n",
        "tokenizer.fit_on_texts(texts)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1"
      ],
      "metadata": {
        "id": "Gh7qVErZWGFp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Sequence Conversion and Padding\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "\"\"\"\n",
        "# 3. Word2Vec Embedding (Simplified Example - In a real scenario, you'd load pre-trained or train your own)\n",
        "embedding_dim = 100  # Dimension of the word embeddings\n",
        "embedding_matrix = np.random.rand(vocab_size, embedding_dim) # Replace with actual Word2Vec embeddings\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "JCvDGe6AWGII",
        "outputId": "5e8b1f92-ddd7-4c32-a93e-248e03d852a7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# 3. Word2Vec Embedding (Simplified Example - In a real scenario, you'd load pre-trained or train your own)\\nembedding_dim = 100  # Dimension of the word embeddings\\nembedding_matrix = np.random.rand(vocab_size, embedding_dim) # Replace with actual Word2Vec embeddings\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "word2vec_model = api.load('word2vec-google-news-300') # Example: loads Google News model\n",
        "word2vec_model.save(\"word2vec-google-news-300.model\") # Save it locally"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjDzdP6TX66y",
        "outputId": "91a85ea8-da97-4d58-ea02-e56fa7f297b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "ozr9u4CjZ5eX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word2vec_model = Word2Vec.load(\"/content/word2vec-google-news-300.model\")\n",
        "# Load the saved KeyedVectors model\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "# Load the saved KeyedVectors model\n",
        "word2vec_model = KeyedVectors.load(\"/content/word2vec-google-news-300.model\", mmap='r')"
      ],
      "metadata": {
        "id": "5NuVnyUjX9oZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Word2Vec Embedding using Gensim\n",
        "\n",
        "# Option 1: Load a pre-trained Word2Vec model (replace with your model path)\n",
        "try:\n",
        "    #word2vec_model = Word2Vec.load(\"path/to/your/pretrained_word2vec.model\")\n",
        "    embedding_dim = word2vec_model.vector_size\n",
        "except FileNotFoundError:\n",
        "    print(\"Pre-trained Word2Vec model not found. Please train one or provide the correct path.\")\n",
        "    exit()\n",
        "\n",
        "# Option 2: Train your own Word2Vec model (uncomment and adjust parameters)\n",
        "# tokenized_texts = [text.split() for text in texts] # Needs more data for meaningful training\n",
        "# embedding_dim = 100\n",
        "# word2vec_model = Word2Vec(tokenized_texts, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "# word2vec_model.save(\"your_trained_word2vec.model\") # Save the trained model\n"
      ],
      "metadata": {
        "id": "EPOJarPjW8-5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    # Check if the word exists directly in the KeyedVectors object\n",
        "    if word in word2vec_model:\n",
        "        # Access the vector directly from the KeyedVectors object\n",
        "        embedding_matrix[i] = word2vec_model[word]\n",
        "\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PJnaE1YXCvA",
        "outputId": "5a260bb8-6052-483d-95fc-2342bbf46408"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding matrix shape: (35, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "OdvqF6xVWGKp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Define the CNN Model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(1, activation='sigmoid') # Binary classification (positive/negative)\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kERsU6B5WGNR",
        "outputId": "849e3d34-fed0-4481-8d22-3f14feb19bea"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Compile the Model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "dOxAJCQdWPJ8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train the Model\n",
        "epochs = 10\n",
        "batch_size = 2\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSOosQz0WPM4",
        "outputId": "54247ab2-945e-4cc5-d44e-3ab25d598d08"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 334ms/step - accuracy: 0.6111 - loss: 0.7029 - val_accuracy: 0.0000e+00 - val_loss: 0.7339\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.2689 - val_accuracy: 0.0000e+00 - val_loss: 0.8726\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.0980 - val_accuracy: 0.0000e+00 - val_loss: 1.0094\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0617 - val_accuracy: 0.0000e+00 - val_loss: 1.1202\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0351 - val_accuracy: 0.0000e+00 - val_loss: 1.2219\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.0154 - val_accuracy: 0.0000e+00 - val_loss: 1.3120\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0099 - val_accuracy: 0.0000e+00 - val_loss: 1.3864\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.0098 - val_accuracy: 0.0000e+00 - val_loss: 1.4491\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 0.0000e+00 - val_loss: 1.5047\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.0000e+00 - val_loss: 1.5527\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ee8b5da7a10>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Evaluate the Model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYffOS2MWPSY",
        "outputId": "bd4aeee9-7c21-49ca-d99e-f2115a0d1840"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.5000 - loss: 1.2828\n",
            "Test Loss: 1.2828\n",
            "Test Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of making predictions\n",
        "new_texts = [\"This is an amazing product!\", \"It was a terrible disappointment.\"]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "new_padded_sequences = pad_sequences(new_sequences, maxlen=max_length, padding='post')\n",
        "predictions = model.predict(new_padded_sequences)\n",
        "for text, pred in zip(new_texts, predictions):\n",
        "    print(f\"Text: '{text}', Prediction (Positive Probability): {pred[0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w85xSVhWGPn",
        "outputId": "caec145b-9799-45ff-fb35-f7f6e88170c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "Text: 'This is an amazing product!', Prediction (Positive Probability): 0.6174\n",
            "Text: 'It was a terrible disappointment.', Prediction (Positive Probability): 0.5788\n"
          ]
        }
      ]
    }
  ]
}