{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding"
      ],
      "metadata": {
        "id": "py5t_shEQrdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- to describe **how words are represented for text analysis**\n",
        "\n",
        "- Typically, this representation takes **the form of a real-valued vector that encodes the word’s meaning**\n",
        "- The expected output is that words close to one another in the vector space will have similar meanings.\n",
        "- Word embeddings can be **created by mapping vocabulary words or phrases to real numbers vectors**.\n",
        "- This is done using various **language modelling and feature-learning techniques**.\n",
        "- **Neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, and explicit representation in terms of word context** are some techniques used to create this mapping.\n",
        "\n",
        "- Using word and phrase embeddings as the underlying input representation has been demonstrated to improve performance in many NLP tasks.\n",
        "- As a result, it’s one of the critical breakthroughs. Combined with deep learning, it has allowed us to solve much more challenging NLP problems.\n",
        "\n",
        "- Word embedding **adds context to words for better automatic language understanding applications**\n"
      ],
      "metadata": {
        "id": "xheT4HjJQrfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Words with the same meaning are represented similarly** in word embedding, a learned representation of text.\n",
        "- One of the significant advances in deep learning for complex natural language processing problems is this method of representing words and documents.\n",
        "\n",
        "- **Individual words are represented as real-valued vectors in a predefined vector space in a technique known as “word embedding.”**\n",
        "- The method is frequently called **“deep learning”** because each word is **assigned to a single vector, and the vector values are learned in a manner resembling a neural network.**\n",
        "\n",
        "- Using a **densely distributed representation for each word** is essential to the method.\n",
        "- A real-valued vector with frequently **tens or hundreds of dimensions represents each word**. In contrast, sparse word representations, like a one-hot encoding or TF-IDF, call for thousands or millions of dimensions.\n",
        "\n",
        "- The word usage and the distributed representation must be learned to derive correct word vectors for word embedding.\n",
        "- Word embeddings make it possible for words used similarly to have representations that **capture their meaning naturally compared to the “bag of words” model, where different terms have different representations regardless of their use.**\n",
        "\n"
      ],
      "metadata": {
        "id": "eBTKlg3rQri5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word embedding algorithms"
      ],
      "metadata": {
        "id": "ap-sOamXQrlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Word2Vec\n",
        "  - Continuous Bag-of-Words (CBOW) model\n",
        "  - Continuous Skip-Gram Model\n",
        "2. GloVe - Global Vectors for Word Representation (GloVe)\n",
        "3. FastText"
      ],
      "metadata": {
        "id": "8fIr1ouXQroL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec"
      ],
      "metadata": {
        "id": "tOdOHxstQrqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A **statistical technique** called Word2Vec can effectively **learn a standalone word embedding from a text corpus**.\n",
        "\n",
        "- It was created by Tomas Mikolov and colleagues at Google in 2013 **to improve the effectiveness of embedding training using neural networks**.\n",
        "- It has since taken over as the industry norm. The work also included **investigating how vector math applied to word representations and analysing the learned vectors.**\n",
        "- A typical example used to explain word vectors is the phrase, **“the king is to the queen as a man is to a woman.**”\n",
        "- If we take the male gender out of the word “king” and add the female gender, we would arrive at the word “queen.”\n",
        "- In this way, we can start to reason with words through the relationships that they hold in regard to other words."
      ],
      "metadata": {
        "id": "3vl6zRyyK1Fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two new learning models were presented **to learn the word embedding using the word2vec method.**\n",
        "\n",
        "1. Continuous Bag-of-Words (CBOW) model\n",
        "    - CBOW model learns the **embedding by predicting the next word based on the current word’s context.**\n",
        "2. Continuous Skip-Gram Model\n",
        "    - learns the embedding for the current word **by predicting the words that will be around it.**\n",
        "\n",
        "Both models emphasise learning words **based on their context, so the words are close by**. A window of nearby words, therefore, **determines the context of a word**. This **window is a model parameter that can be adjusted according to a given use case.**\n",
        "\n",
        "The method’s main advantage is **its ability to learn high-quality word embeddings quickly**, **enabling the learning of more significant embeddings from high-dimensional data**. As a result, much larger corpora of text with billions of words can be easily represented.\n"
      ],
      "metadata": {
        "id": "xoZ2-yD8K1IO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe - Global Vectors for Word Representation (GloVe)"
      ],
      "metadata": {
        "id": "O-ran7OfK1K_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "T- he word2vec algorithm has been extended to create the **Global Vectors for Word Representation (GloVe) algorithm**.\n",
        "- GloVe is based on **word-context matrix factorisation techniques**.\n",
        "- It first creates a **sizable matrix of (words x context) co-occurrence data**, in which you count the number of times a word appears in a particular “context” (the columns) for each “word” (the rows).\n",
        "- There are many “contexts” because their size is essentially combinatorial. When this matrix is **factorised, a lower-dimensional (words x features) matrix is produced, with each row creating a vector representation for the corresponding word.**\n",
        "- Typically, this is accomplished by reducing a **“reconstruction loss.”**\n",
        "- This loss looks **for lower-dimensional models that can account for the majority of the variance in the high-dimensional data**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kE1ExF3CK1NO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec Vs GloVe**\n",
        "- - GloVe creates an **explicit word context or word co-occurrence matrix using statistics across the entire text corpus rather than using a window to define local context**, like in Word2Vec. The outcome is a learning model that might lead to more effective word embeddings."
      ],
      "metadata": {
        "id": "jKMDf2mtK1P3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastText"
      ],
      "metadata": {
        "id": "u-LfkqrmVv4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- FastText, essentially a word2vec model extension, **treats each word as being made up of character n-grams**.\n",
        "- Thus, the sum of these character **n-grams constitutes the vector for a word**. For instance, the word vector “orange” is the sum of the n-gram vectors:\n",
        "\n",
        "\"<or\", \"ora\", \"oran\", \"orang\", \"orange\" \"orange>\", \"ran\", \"rang\", \"range\" \"range>\", \"ang\", \"ange\", \"ange>\", \"nge\",\"nge>\", \"ge\", \"ge>\"\n",
        "\n",
        "- use of n-grams is the **primary distinction between FastText and Word2Vec**.\n",
        "- **Word2Vec uses only complete words found in the training corpus to learn vectors**. In contrast, **FastText learns vectors for individual words and the n-grams found within them**. The mean of the target word vector and its n-gram component vectors are used for training at each stage of the FastText process.\n",
        "- **Each combined vector creates the target**, which is then uniformly updated using the adjustment calculated from the error. These calculations significantly **increase the amount of computation in the training phase**. A word must add up and average its n-gram parts at each point.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y9HDeZ44Vv7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The most **notable enhancement to FastText is the N-gram feature, which addresses the OOV (out-of-vocabulary) problem**. For instance, the word “aquarium” can be broken down into “aq/aqu/qua/uar/ari/riu/ium/um>,” where “<” and “>” denote the beginning and end of the word, respectively. Though the word embedder may not immediately recognise the word “Aquarius,” it can infer its meaning. This can be done because the words “aquarium” and “Aquarius” share a common root."
      ],
      "metadata": {
        "id": "q4o20hxWWHP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learn word embedding"
      ],
      "metadata": {
        "id": "xwjTpb1jVv-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning your embeddings is a good solution when you have a training data set and the computational resources to train the model. Going down this route has the advantage of training your model to optimise it for your use case. If this is done correctly, you can yield far better results than a pre-trained model. When developing your word embedding, you have two primary choices:\n",
        "\n",
        "- Learn it **independently**, in which case a model is trained to learn the embedding, which is then saved and used as a component of another model for your task in the future. This is a good strategy for using the same embedding in various models.\n",
        "- Learn it **jointly**, where the embedding is learned as a component of a sizable model tailored to a given task. This is a good strategy if you only use the embedding for one task.\n"
      ],
      "metadata": {
        "id": "XULmnM2JVwDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Re-use existing word embeddings"
      ],
      "metadata": {
        "id": "kOtqyc9EVwIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-trained word embeddings are frequently made freely available by researchers under a permissive license so that you can use them in your research or business endeavours. For instance, word2vec and GloVe word embeddings can be downloaded free of charge. So instead of creating your embeddings from scratch, you can use these in your project. When it comes to using pre-trained embeddings, you have two primary choices:\n",
        "\n",
        "- The static option. This means that the embedding is used as part of your model but is kept static. This strategy is appropriate if the embedding fits your issue well and produces valuable results.\n",
        "- The update option. This is where the model is seeded with the previously trained embedding, but the embedding is jointly updated throughout the model training process. This might be a good option if you want to make the most of the model by embedding it in your task.\n"
      ],
      "metadata": {
        "id": "ZEYgir5pQrtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary"
      ],
      "metadata": {
        "id": "92lxjBLCXiD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Word embeddings have revolutionised the world of natural language processing. We can now reason with text in a way that was impossible before with a bag of words or the TF-IDF word vectorisation technique.\n",
        "- There are three main word embedding algorithms; word2vec, GloVe, and FastText. All three have slightly different implementations and have their advantages and disadvantages. Understanding these differences will let you choose the correct algorithm for your task.\n",
        "- Depending on your problem, data, and the processing power available to train a model, you might train your embeddings or use a pre-trained model instead.\n"
      ],
      "metadata": {
        "id": "f9rxdMFcXjGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source"
      ],
      "metadata": {
        "id": "S5oyZJsgQrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://spotintelligence.com/2022/11/30/word-embedding/"
      ],
      "metadata": {
        "id": "3gq7kBVnQryJ"
      }
    }
  ]
}