{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec for text classification"
      ],
      "metadata": {
        "id": "HhMUwinVZAsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Word2Vec is a popular algorithm used for **natural language processing and text classification**.\n",
        "- It is a **neural network-based approach** that learns **distributed representations (also called embeddings)** of words from a large corpus of text.\n",
        "- These embeddings capture the semantic and syntactic relationships between terms, which can be used to improve text classification accuracy"
      ],
      "metadata": {
        "id": "vK2MwMwPZAv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approaches"
      ],
      "metadata": {
        "id": "l63DNzJQZAy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approach 1:**\n",
        "- using Word2Vec for text classification is to train the Word2Vec model on a **large text dataset**. This can be done using a tool like **Gensim or TensorFlow**. Then, once the embeddings have been introduced, they can be used as features in a machine learning model for text classification.\n",
        "\n",
        "- represent each document as a vector by taking the average of the Word2Vec embeddings of the words in the document. Then, a classifier like logistic regression, random forests, or support vector machines can take this vector as an input.\n",
        "\n",
        "**Approach 2:**\n",
        "- **fine-tune the Word2Vec embeddings** on specific task by training a **neural network to classify the text**. In this approach, the Word2Vec embeddings are used as the input to the neural network, which is then trained to predict the class labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "xHWt7QYRZA1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "00hrG3AFZA4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HlsMYyd3ZA62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF vs Word2Vec"
      ],
      "metadata": {
        "id": "m5Kt4p0VZA9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**\n",
        "- widely used method for **text representation**.\n",
        "- It assigns weights to words in a document based on their frequency in the document and inverse frequency in the corpus.\n",
        "- The idea is that words that are frequent in a document but rare in the corpus are likely to be important for that documentâ€™s meaning. This approach is commonly used for **information retrieval and text classification tasks**.\n",
        "- TF-IDF is a good choice when the documents in the dataset are relatively short and the vocabulary size is small. It is also computationally efficient and can handle large datasets but don not capture the meaning of words\n",
        "\n"
      ],
      "metadata": {
        "id": "hFryXxYiZBAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec**\n",
        "\n",
        "- more complex algorithm that **learns vector representations (embeddings) of words based on their context in a large corpus of text**.\n",
        "- These embeddings show how words are related semantically and grammatically.\n",
        "- They can be used for various natural language processing tasks, such as finding **word analogies and similar words and putting texts into groups**.\n",
        "- Word2Vec is better suited for **larger and more complex datasets**, where words have multiple meanings and relationships between them are essential.\n",
        "- It is particularly effective when the task involves **identifying similarities or relationships between documents, such as clustering or document retrieval**.\n",
        "- However, Word2Vec requires a large amount of data and computational resources to train, and it may perform poorly when limited training data is available.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OxgIrZzKZBCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised text classification Word2Vec"
      ],
      "metadata": {
        "id": "1Dw1wo4dZBFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Unsupervised text classification using Word2Vec involves using the Word2Vec embeddings of words to group similar documents together without needing labelled data\n",
        "- **Approach 1: **\n",
        "  - to train the Word2Vec model on a large corpus of text and then represent each document as a vector by taking the average of the Word2Vec embeddings of the words in the document. This **results in a vector representation of the document that captures the meaning and context of the words in the document.**\n",
        "  - Once the documents have been represented as vectors, clustering algorithms such as K-means, hierarchical clustering, or density-based clustering can be used to group similar documents. The clusters can then be looked at to see if they are about different things.\n",
        "\n",
        "- **Approach 2:**\n",
        "  - to use Word2Vec embeddings **to identify similar documents using techniques such as cosine similarity or euclidean distance**\n",
        "  - each document is represented as a **vector** using the Word2Vec embeddings, and then the **similarity between pairs of documents is computed**. **Documents with high similarity scores are then grouped**\n",
        "\n",
        "Unsupervised text classification using Word2Vec can be a powerful tool for **discovering latent themes and patterns in large amounts of unstructured text data**. However, the quality of the clustering or similarity results depends on the quality of the Word2Vec embeddings and the clustering algorithm used"
      ],
      "metadata": {
        "id": "W3IHh2XObI52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Steps"
      ],
      "metadata": {
        "id": "iXP_7u_3bI85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset of movie reviews, where each review is labelled as either positive or negative. the task is to build a classifier that can predict the sentiment of a new review as either positive or negative.\n",
        "\n"
      ],
      "metadata": {
        "id": "o1hskZUJbJBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Pre-processing the text data:**\n",
        "- to pre-process the text data by removing **stop words, converting all text to lowercase, and removing punctuation**. use tools such as **NLTK or spaCy** for this.\n",
        "\n",
        "**2.Training the Word2Vec model:**\n",
        "- Once the data has been pre-processed, you can train a Word2Vec model using a tool such as **Gensim**. The model figures out how to represent words as vectors based on their use in the movie reviews dataset.\n",
        "\n",
        "**3.Vectorizing the movie reviews:** After training the Word2Vec model, you can represent each movie review as a **vector by taking the average of the Word2Vec embeddings of the words in the review**. This makes a vector representation of the review that shows how the words fit together and what they mean.\n",
        "\n",
        "**4.Splitting the data:** Split the dataset into training and testing sets.\n",
        "\n",
        "**5.Building a classifier:** Train a machine learning model such as **logistic regression, random forests, or support vector machines** using the vector representations of the movie reviews as input features and the sentiment labels as the target variable.\n",
        "\n",
        "**6.Evaluating the model:** Once the model has been trained, use metrics like accuracy, precision, recall, and F1-score to judge how well it did on the testing set."
      ],
      "metadata": {
        "id": "A7_1v-0gbJED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source"
      ],
      "metadata": {
        "id": "jQjGKyZgbJX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://spotintelligence.com/2023/02/15/word2vec-for-text-classification/"
      ],
      "metadata": {
        "id": "UGsnLmJAbLBO"
      }
    }
  ]
}