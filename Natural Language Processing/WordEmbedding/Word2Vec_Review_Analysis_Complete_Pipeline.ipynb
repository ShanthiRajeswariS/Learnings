{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJIkJPjCRMzd",
        "outputId": "df03c60d-5dfa-4d87-be06-cbfed58cd183"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G_Rsk9CVQ9LW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbFyIGckQ983",
        "outputId": "85c03be2-2f87-459f-85ce-7ec9e90a5295"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt.tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_7GySVjReDM",
        "outputId": "c6c7a32e-e791-4511-b968-b890c17fad62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading punkt.tab: Package 'punkt.tab' not found in\n",
            "[nltk_data]     index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "omBia_R5Q9_w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sample_data(n_samples=2000):\n",
        "    np.random.seed(42)\n",
        "    positive_phrases = [\n",
        "        \"loved it\", \"fantastic movie\", \"excellent acting\", \"brilliant performance\",\n",
        "        \"amazing plot\", \"great direction\", \"wonderful cinematography\", \"highly recommend\",\n",
        "        \"touching story\", \"emotional journey\", \"masterpiece\", \"outstanding film\",\n",
        "        \"captivating storyline\", \"superb cast\", \"visually stunning\"\n",
        "    ]\n",
        "\n",
        "    negative_phrases = [\n",
        "        \"waste of time\", \"terrible acting\", \"awful script\", \"poorly directed\",\n",
        "        \"boring plot\", \"disappointing ending\", \"bad cinematography\", \"wouldn't recommend\",\n",
        "        \"predictable story\", \"wooden performances\", \"lackluster\", \"dreadful film\",\n",
        "        \"uninspired storyline\", \"mediocre cast\", \"visually dull\"\n",
        "    ]\n",
        "\n",
        "    reviews = []\n",
        "    sentiments = []\n",
        "\n",
        "    for _ in range(n_samples // 2):\n",
        "        # Generate positive review\n",
        "        n_positive = np.random.randint(1, 4)\n",
        "        positive_indices = np.random.choice(len(positive_phrases), n_positive, replace=False)\n",
        "        pos_review = \" \".join([positive_phrases[i] for i in positive_indices])\n",
        "        filler_words = np.random.randint(5, 20)\n",
        "        pos_review = f\"I watched this movie last weekend. {pos_review} It was a {np.random.choice(['good', 'great', 'fantastic'])} experience.\"\n",
        "        reviews.append(pos_review)\n",
        "        sentiments.append(\"positive\")\n",
        "\n",
        "        # Generate negative review\n",
        "        n_negative = np.random.randint(1, 4)\n",
        "        negative_indices = np.random.choice(len(negative_phrases), n_negative, replace=False)\n",
        "        neg_review = \" \".join([negative_phrases[i] for i in negative_indices])\n",
        "        filler_words = np.random.randint(5, 20)\n",
        "        neg_review = f\"I saw this film recently. {neg_review} It was a {np.random.choice(['bad', 'terrible', 'disappointing'])} experience.\"\n",
        "        reviews.append(neg_review)\n",
        "        sentiments.append(\"negative\")\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'review': reviews,\n",
        "        'sentiment': sentiments\n",
        "    })\n",
        "\n",
        "    # Shuffle the data\n",
        "    return df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "W5v9X3XHQ-CW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create or load data\n",
        "df = create_sample_data(n_samples=2000)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(df.head())\n",
        "\n",
        "# Check class distribution\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df['sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qVcuWpIQ-Ht",
        "outputId": "c5a84a39-b18b-41f6-edf7-11f81ff9cd26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (2000, 2)\n",
            "                                              review sentiment\n",
            "0  I saw this film recently. disappointing ending...  negative\n",
            "1  I watched this movie last weekend. masterpiece...  positive\n",
            "2  I saw this film recently. waste of time It was...  negative\n",
            "3  I saw this film recently. wooden performances ...  negative\n",
            "4  I saw this film recently. boring plot dreadful...  negative\n",
            "\n",
            "Class distribution:\n",
            "sentiment\n",
            "negative    1000\n",
            "positive    1000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess text data\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Remove short words (length < 3)\n",
        "    tokens = [word for word in tokens if len(word) >= 3]\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "PD66SqQOQ-Kj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wnUmpfyS2QO",
        "outputId": "eeceb1dd-849c-43b1-ee23-7a45b8a2c8f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "print(\"\\nPreprocessing text data...\")\n",
        "start_time = time.time()\n",
        "df['processed_text'] = df['review'].apply(preprocess_text)\n",
        "print(f\"Preprocessing completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Check a sample of processed text\n",
        "print(\"\\nSample of processed text:\")\n",
        "print(df['processed_text'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWk4uAUJQ-NN",
        "outputId": "0d0b3967-614a-4c73-b814-d60c51a80229"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessing text data...\n",
            "Preprocessing completed in 4.99 seconds\n",
            "\n",
            "Sample of processed text:\n",
            "0    [saw, film, recently, disappointing, ending, w...\n",
            "1    [watched, movie, last, weekend, masterpiece, g...\n",
            "2    [saw, film, recently, waste, time, bad, experi...\n",
            "3    [saw, film, recently, wooden, performance, ter...\n",
            "4    [saw, film, recently, boring, plot, dreadful, ...\n",
            "Name: processed_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "print(\"\\nTraining Word2Vec model...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Set Word2Vec parameters\n",
        "vector_size = 100  # Dimensionality of word vectors\n",
        "window_size = 5    # Context window size\n",
        "min_count = 2      # Minimum word count threshold\n",
        "sg = 1             # Training algorithm: 1 for skip-gram; 0 for CBOW\n",
        "\n",
        "# Train model\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=df['processed_text'].tolist(),\n",
        "    vector_size=vector_size,\n",
        "    window=window_size,\n",
        "    min_count=min_count,\n",
        "    sg=sg,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "print(f\"Word2Vec model trained in {time.time() - start_time:.2f} seconds\")\n",
        "print(f\"Vocabulary size: {len(w2v_model.wv.key_to_index)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x41vyHoRRss0",
        "outputId": "7e316ba7-8998-46f3-e2b4-066e010e94da"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Word2Vec model...\n",
            "Word2Vec model trained in 0.09 seconds\n",
            "Vocabulary size: 54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "w2v_model.save(\"movie_reviews_word2vec.model\")\n",
        "print(\"Word2Vec model saved to disk\")\n",
        "\n",
        "# Alternatively, we could load pre-trained vectors:\n",
        "# w2v_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOeg1hWuRsvm",
        "outputId": "a3b27014-8d7c-44fa-d447-fca12ac20c66"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec model saved to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction: Create document vectors\n",
        "def document_vector(doc, model):\n",
        "    \"\"\"Create document vectors by averaging word vectors\"\"\"\n",
        "    # Filter words in vocabulary\n",
        "    doc_words = [word for word in doc if word in model.wv]\n",
        "\n",
        "    if len(doc_words) == 0:\n",
        "        # If no words are in vocabulary, return zero vector\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    # Return average of word vectors\n",
        "    return np.mean([model.wv[word] for word in doc_words], axis=0)\n"
      ],
      "metadata": {
        "id": "IuZtsBn7RsyO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert documents to vectors\n",
        "print(\"\\nConverting documents to feature vectors...\")\n",
        "start_time = time.time()\n",
        "df['document_vector'] = df['processed_text'].apply(lambda x: document_vector(x, w2v_model))\n",
        "print(f\"Document vectors created in {time.time() - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrNez6imRs0u",
        "outputId": "fcb0ba87-3abb-4413-bd08-d3bcbc80853c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Converting documents to feature vectors...\n",
            "Document vectors created in 0.07 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature matrix and target variable\n",
        "X = np.array(df['document_vector'].tolist())\n",
        "y = df['sentiment'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AAKXzSCRs3T",
        "outputId": "06dbda15-8e3b-4530-d91e-75705f13352b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training set shape: (1600, 100)\n",
            "Testing set shape: (400, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare different classifiers\n",
        "classifiers = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'SVM': SVC(kernel='linear', random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "PqzRAK1pRs5p"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n"
      ],
      "metadata": {
        "id": "5_2v-ASeRs8O"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTraining and evaluating classifiers:\")\n",
        "for name, clf in classifiers.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    start_time = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    # Predict and evaluate\n",
        "    start_time = time.time()\n",
        "    y_pred = clf.predict(X_test)\n",
        "    predict_time = time.time() - start_time\n",
        "\n",
        "    # Store results\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'training_time': train_time,\n",
        "        'prediction_time': predict_time,\n",
        "        'classifier': clf,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "\n",
        "    print(f\"{name} accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Training time: {train_time:.2f} seconds\")\n",
        "    print(f\"Prediction time: {predict_time:.2f} seconds\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msA9-WMDRs_E",
        "outputId": "7d963107-0a86-4d23-847f-b0c14aa328f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and evaluating classifiers:\n",
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression accuracy: 1.0000\n",
            "Training time: 0.02 seconds\n",
            "Prediction time: 0.00 seconds\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00       200\n",
            "    positive       1.00      1.00      1.00       200\n",
            "\n",
            "    accuracy                           1.00       400\n",
            "   macro avg       1.00      1.00      1.00       400\n",
            "weighted avg       1.00      1.00      1.00       400\n",
            "\n",
            "\n",
            "Training SVM...\n",
            "SVM accuracy: 1.0000\n",
            "Training time: 0.01 seconds\n",
            "Prediction time: 0.00 seconds\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00       200\n",
            "    positive       1.00      1.00      1.00       200\n",
            "\n",
            "    accuracy                           1.00       400\n",
            "   macro avg       1.00      1.00      1.00       400\n",
            "weighted avg       1.00      1.00      1.00       400\n",
            "\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest accuracy: 1.0000\n",
            "Training time: 0.34 seconds\n",
            "Prediction time: 0.01 seconds\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00       200\n",
            "    positive       1.00      1.00      1.00       200\n",
            "\n",
            "    accuracy                           1.00       400\n",
            "   macro avg       1.00      1.00      1.00       400\n",
            "weighted avg       1.00      1.00      1.00       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best classifier\n",
        "best_clf_name = max(results, key=lambda x: results[x]['accuracy'])\n",
        "best_clf = results[best_clf_name]['classifier']\n",
        "best_accuracy = results[best_clf_name]['accuracy']\n",
        "\n",
        "print(f\"\\nBest classifier: {best_clf_name} with accuracy: {best_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OTz35OwRtBY",
        "outputId": "10f07157-be2e-445d-fdab-4c5b53193c21"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best classifier: Logistic Regression with accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize confusion matrix for the best classifier\n",
        "best_predictions = results[best_clf_name]['predictions']\n",
        "cm = confusion_matrix(y_test, best_predictions)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Confusion Matrix - {best_clf_name}')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "print(\"\\nConfusion matrix saved as 'confusion_matrix.png'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxH-afJIQ-Pz",
        "outputId": "cca5b425-9c49-48a2-9aa9-5fe00aaaf9c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion matrix saved as 'confusion_matrix.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model\n",
        "with open(f\"sentiment_classifier_{best_clf_name.replace(' ', '_').lower()}.pkl\", \"wb\") as f:\n",
        "    pickle.dump(best_clf, f)\n",
        "print(f\"Best model saved as 'sentiment_classifier_{best_clf_name.replace(' ', '_').lower()}.pkl'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik851CEtSDeo",
        "outputId": "ef313906-3f43-4a2a-b4db-f1bd21a81124"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved as 'sentiment_classifier_logistic_regression.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for the best classifier\n",
        "print(\"\\nPerforming hyperparameter tuning for the best classifier...\")\n",
        "\n",
        "if best_clf_name == \"Logistic Regression\":\n",
        "    param_grid = {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    }\n",
        "elif best_clf_name == \"SVM\":\n",
        "    param_grid = {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'kernel': ['linear', 'rbf'],\n",
        "        'gamma': ['scale', 'auto', 0.1, 1]\n",
        "    }\n",
        "else:  # Random Forest\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27DTEOBiSDhY",
        "outputId": "c4cb628d-a776-4f34-bca7-b67fc0de1af4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performing hyperparameter tuning for the best classifier...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform grid search\n",
        "grid_search = GridSearchCV(best_clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_1NgCKSSDkp",
        "outputId": "6907c460-ab75-4348-acb9-02e12339a999"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best cross-validation accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate tuned model\n",
        "tuned_clf = grid_search.best_estimator_\n",
        "y_pred_tuned = tuned_clf.predict(X_test)\n",
        "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
        "\n",
        "print(f\"\\nTuned model accuracy: {tuned_accuracy:.4f}\")\n",
        "print(\"\\nClassification Report for tuned model:\")\n",
        "print(classification_report(y_test, y_pred_tuned))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnzntCT6SDui",
        "outputId": "e41f9250-f0dd-4b14-c024-d749f0535c81"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tuned model accuracy: 1.0000\n",
            "\n",
            "Classification Report for tuned model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00       200\n",
            "    positive       1.00      1.00      1.00       200\n",
            "\n",
            "    accuracy                           1.00       400\n",
            "   macro avg       1.00      1.00      1.00       400\n",
            "weighted avg       1.00      1.00      1.00       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tuned model\n",
        "with open(f\"tuned_sentiment_classifier_{best_clf_name.replace(' ', '_').lower()}.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tuned_clf, f)\n",
        "print(f\"Tuned model saved as 'tuned_sentiment_classifier_{best_clf_name.replace(' ', '_').lower()}.pkl'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdfQA4C1SK3V",
        "outputId": "f365b5e0-2499-43c7-92c4-b82ef0a79fe9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuned model saved as 'tuned_sentiment_classifier_logistic_regression.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to predict sentiment for new reviews\n",
        "def predict_sentiment(review_text, w2v_model=w2v_model, classifier=tuned_clf):\n",
        "    \"\"\"Predict sentiment for a new movie review\"\"\"\n",
        "    # Preprocess the review\n",
        "    processed_review = preprocess_text(review_text)\n",
        "\n",
        "    # Convert to document vector\n",
        "    review_vector = document_vector(processed_review, w2v_model)\n",
        "\n",
        "    # Reshape for prediction\n",
        "    review_vector = review_vector.reshape(1, -1)\n",
        "\n",
        "    # Predict sentiment\n",
        "    prediction = classifier.predict(review_vector)[0]\n",
        "\n",
        "    # Get probability scores if classifier supports it\n",
        "    try:\n",
        "        probability = classifier.predict_proba(review_vector)[0]\n",
        "        confidence = max(probability)\n",
        "    except:\n",
        "        # Some classifiers like SVM don't have predict_proba\n",
        "        confidence = None\n",
        "\n",
        "    return {\n",
        "        'sentiment': prediction,\n",
        "        'confidence': confidence\n",
        "    }"
      ],
      "metadata": {
        "id": "retemk-_SK6L"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the sentiment prediction function\n",
        "example_reviews = [\n",
        "    \"This movie was absolutely fantastic! The director did an amazing job.\",\n",
        "    \"I found the plot boring and the acting was terrible.\",\n",
        "    \"The movie was okay, not great but not terrible either.\",\n",
        "    \"A masterpiece that will stand the test of time. Incredible performances by all.\"\n",
        "]"
      ],
      "metadata": {
        "id": "GMfp-NnWSK9G"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPredicting sentiment for example reviews:\")\n",
        "for review in example_reviews:\n",
        "    result = predict_sentiment(review)\n",
        "    print(f\"\\nReview: {review}\")\n",
        "    print(f\"Predicted sentiment: {result['sentiment']}\")\n",
        "    if result['confidence'] is not None:\n",
        "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLKZni3pSK_-",
        "outputId": "a619fefa-d4ca-4001-d182-93346cbaa5cd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicting sentiment for example reviews:\n",
            "\n",
            "Review: This movie was absolutely fantastic! The director did an amazing job.\n",
            "Predicted sentiment: positive\n",
            "Confidence: 0.8148\n",
            "\n",
            "Review: I found the plot boring and the acting was terrible.\n",
            "Predicted sentiment: negative\n",
            "Confidence: 0.5886\n",
            "\n",
            "Review: The movie was okay, not great but not terrible either.\n",
            "Predicted sentiment: positive\n",
            "Confidence: 0.6781\n",
            "\n",
            "Review: A masterpiece that will stand the test of time. Incredible performances by all.\n",
            "Predicted sentiment: positive\n",
            "Confidence: 0.5294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple evaluation function to assess the model's performance\n",
        "# on different types of reviews\n",
        "def evaluate_model_on_edge_cases():\n",
        "    \"\"\"Test the model on specific edge cases and challenging reviews\"\"\"\n",
        "    edge_cases = [\n",
        "        # Mixed sentiment\n",
        "        \"The movie had great acting but the plot was confusing.\",\n",
        "        \"Brilliant special effects couldn't save the terrible storyline.\",\n",
        "\n",
        "        # Negation\n",
        "        \"This movie wasn't as bad as people say.\",\n",
        "        \"The film wasn't great but it wasn't terrible either.\",\n",
        "\n",
        "        # Sarcasm\n",
        "        \"Oh sure, just what we needed, another superhero movie.\",\n",
        "        \"If you enjoy watching paint dry, you'll love this film.\",\n",
        "\n",
        "        # Short reviews\n",
        "        \"Loved it!\",\n",
        "        \"Terrible.\",\n",
        "\n",
        "        # Implicit sentiment\n",
        "        \"I fell asleep halfway through.\",\n",
        "        \"I've watched it three times already.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nEvaluating model on challenging reviews:\")\n",
        "    for review in edge_cases:\n",
        "        result = predict_sentiment(review)\n",
        "        print(f\"\\nReview: {review}\")\n",
        "        print(f\"Predicted sentiment: {result['sentiment']}\")\n",
        "        if result['confidence'] is not None:\n",
        "            print(f\"Confidence: {result['confidence']:.4f}\")\n"
      ],
      "metadata": {
        "id": "tffsVrq0SLCt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the edge case evaluation\n",
        "evaluate_model_on_edge_cases()\n",
        "\n",
        "print(\"\\nExploring the Word2Vec model:\")\n",
        "# Find most similar words to positive sentiment words\n",
        "positive_words = [\"excellent\", \"amazing\", \"great\"]\n",
        "print(\"\\nMost similar words to positive sentiment words:\")\n",
        "for word in positive_words:\n",
        "    if word in w2v_model.wv:\n",
        "        similar_words = w2v_model.wv.most_similar(word, topn=5)\n",
        "        print(f\"\\nWords similar to '{word}':\")\n",
        "        for similar_word, similarity in similar_words:\n",
        "            print(f\"  {similar_word}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\nWord '{word}' not in vocabulary\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hrkwOUGSLFX",
        "outputId": "2b76a9af-1893-4af2-97dd-244fa8031ba1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model on challenging reviews:\n",
            "\n",
            "Review: The movie had great acting but the plot was confusing.\n",
            "Predicted sentiment: positive\n",
            "Confidence: 0.7197\n",
            "\n",
            "Review: Brilliant special effects couldn't save the terrible storyline.\n",
            "Predicted sentiment: positive\n",
            "Confidence: 0.5316\n",
            "\n",
            "Review: This movie wasn't as bad as people say.\n",
            "Predicted sentiment: positive\n",
            "Confidence: 0.5909\n",
            "\n",
            "Review: The film wasn't great but it wasn't terrible either.\n",
            "Predicted sentiment: negative\n",
            "Confidence: 0.5399\n",
            "\n",
            "Review: Oh sure, just what we needed, another superhero movie.\n",
            "Predicted sentiment: positive\n",
            "Confidence: 0.8465\n",
            "\n",
            "Review: If you enjoy watching paint dry, you'll love this film.\n",
            "Predicted sentiment: negative\n",
            "Confidence: 0.7327\n",
            "\n",
            "Review: Loved it!\n",
            "Predicted sentiment: positive\n",
            "Confidence: 0.7733\n",
            "\n",
            "Review: Terrible.\n",
            "Predicted sentiment: negative\n",
            "Confidence: 0.7377\n",
            "\n",
            "Review: I fell asleep halfway through.\n",
            "Predicted sentiment: negative\n",
            "Confidence: 0.5000\n",
            "\n",
            "Review: I've watched it three times already.\n",
            "Predicted sentiment: positive\n",
            "Confidence: 0.5749\n",
            "\n",
            "Exploring the Word2Vec model:\n",
            "\n",
            "Most similar words to positive sentiment words:\n",
            "\n",
            "Words similar to 'excellent':\n",
            "  loved: 0.9974\n",
            "  good: 0.9971\n",
            "  direction: 0.9970\n",
            "  masterpiece: 0.9969\n",
            "  brilliant: 0.9967\n",
            "\n",
            "Words similar to 'amazing':\n",
            "  masterpiece: 0.9967\n",
            "  loved: 0.9965\n",
            "  excellent: 0.9961\n",
            "  outstanding: 0.9959\n",
            "  good: 0.9958\n",
            "\n",
            "Words similar to 'great':\n",
            "  fantastic: 0.9987\n",
            "  good: 0.9981\n",
            "  masterpiece: 0.9979\n",
            "  last: 0.9978\n",
            "  watched: 0.9977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find most similar words to negative sentiment words\n",
        "negative_words = [\"terrible\", \"boring\", \"awful\"]\n",
        "print(\"\\nMost similar words to negative sentiment words:\")\n",
        "for word in negative_words:\n",
        "    if word in w2v_model.wv:\n",
        "        similar_words = w2v_model.wv.most_similar(word, topn=5)\n",
        "        print(f\"\\nWords similar to '{word}':\")\n",
        "        for similar_word, similarity in similar_words:\n",
        "            print(f\"  {similar_word}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\nWord '{word}' not in vocabulary\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ3D2xE4SLH8",
        "outputId": "d24037b8-3741-4ee3-acfc-8470bfb418a1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most similar words to negative sentiment words:\n",
            "\n",
            "Words similar to 'terrible':\n",
            "  lackluster: 0.9981\n",
            "  bad: 0.9979\n",
            "  disappointing: 0.9975\n",
            "  boring: 0.9969\n",
            "  film: 0.9968\n",
            "\n",
            "Words similar to 'boring':\n",
            "  lackluster: 0.9982\n",
            "  bad: 0.9972\n",
            "  terrible: 0.9969\n",
            "  wooden: 0.9966\n",
            "  dreadful: 0.9966\n",
            "\n",
            "Words similar to 'awful':\n",
            "  saw: 0.9968\n",
            "  dreadful: 0.9966\n",
            "  disappointing: 0.9960\n",
            "  recently: 0.9957\n",
            "  ending: 0.9955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word analogies\n",
        "try:\n",
        "    print(\"\\nWord analogies:\")\n",
        "    # Classic example: king - man + woman = queen\n",
        "    result = w2v_model.wv.most_similar(positive=['good', 'terrible'], negative=['bad'], topn=1)\n",
        "    print(f\"good - bad + terrible = {result[0][0]} (similarity: {result[0][1]:.4f})\")\n",
        "except:\n",
        "    print(\"\\nCould not perform word analogies (vocabulary might be too small)\")\n",
        "\n",
        "print(\"\\nModel Pipeline Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpkHvvyKSLKl",
        "outputId": "f03a9a5a-e59b-4a1f-f91a-10a5b0517568"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word analogies:\n",
            "good - bad + terrible = excellent (similarity: 0.9967)\n",
            "\n",
            "Model Pipeline Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSR9yus1SLNL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}