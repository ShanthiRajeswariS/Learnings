{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram"
      ],
      "metadata": {
        "id": "pUJitngFVXFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.stats import spearmanr\n",
        "import re\n",
        "import time\n"
      ],
      "metadata": {
        "id": "WkjIf0QfQg_h"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram:\n",
        "    def __init__(self, text=None, embedding_dim=100, window_size=2, negative_samples=5,\n",
        "                 min_count=5, subsample_threshold=1e-5, learning_rate=0.025):\n",
        "        \"\"\"\n",
        "        Initialize the Skip-Gram model\n",
        "\n",
        "        Args:\n",
        "            text (str): Text corpus to train on\n",
        "            embedding_dim (int): Dimension of word embeddings\n",
        "            window_size (int): Context window size (on each side)\n",
        "            negative_samples (int): Number of negative samples per positive sample\n",
        "            min_count (int): Minimum word frequency to be included in vocabulary\n",
        "            subsample_threshold (float): Threshold for subsampling frequent words\n",
        "            learning_rate (float): Initial learning rate\n",
        "        \"\"\"\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_size = window_size\n",
        "        self.negative_samples = negative_samples\n",
        "        self.min_count = min_count\n",
        "        self.subsample_threshold = subsample_threshold\n",
        "        self.learning_rate = learning_rate\n",
        "        self.initial_lr = learning_rate\n",
        "\n",
        "        # Initialize vocabulary and embeddings if text is provided\n",
        "        if text:\n",
        "            self.build_vocab(text)\n",
        "            self.initialize_embeddings()\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Convert text to lowercase and split into tokens\"\"\"\n",
        "        return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "    def build_vocab(self, text):\n",
        "        \"\"\"\n",
        "        Build vocabulary from text corpus\n",
        "\n",
        "        Args:\n",
        "            text (str): Text corpus\n",
        "        \"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "\n",
        "        # Count word frequencies\n",
        "        word_counts = Counter(tokens)\n",
        "        total_words = sum(word_counts.values())\n",
        "\n",
        "        # Filter words below min_count\n",
        "        filtered_words = {word: count for word, count in word_counts.items()\n",
        "                         if count >= self.min_count}\n",
        "\n",
        "        # Create word-to-index and index-to-word mappings\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.word_counts = {}\n",
        "        self.discard_probs = {}  # For subsampling\n",
        "\n",
        "        # Add words to vocabulary\n",
        "        for idx, (word, count) in enumerate(filtered_words.items()):\n",
        "            self.word2idx[word] = idx\n",
        "            self.idx2word[idx] = word\n",
        "            self.word_counts[word] = count\n",
        "\n",
        "            # Calculate probability for subsampling frequent words\n",
        "            # Formula from original word2vec paper\n",
        "            freq = count / total_words\n",
        "            self.discard_probs[word] = 1 - np.sqrt(self.subsample_threshold / freq)\n",
        "\n",
        "        self.vocab_size = len(self.word2idx)\n",
        "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
        "\n",
        "        # Create dataset of word indices\n",
        "        self.data = [self.word2idx[word] for word in tokens if word in self.word2idx]\n",
        "        print(f\"Total words in dataset after filtering: {len(self.data)}\")\n",
        "\n",
        "    def initialize_embeddings(self):\n",
        "        \"\"\"Initialize input and output embeddings\"\"\"\n",
        "        # Initialize weights with small random values\n",
        "        self.W_input = np.random.uniform(-0.5/self.embedding_dim, 0.5/self.embedding_dim,\n",
        "                                         (self.vocab_size, self.embedding_dim))\n",
        "        self.W_output = np.zeros((self.vocab_size, self.embedding_dim))\n",
        "\n",
        "    def get_target_contexts(self, idx):\n",
        "        \"\"\"\n",
        "        Get context words for a given center word position\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the center word in the dataset\n",
        "\n",
        "        Returns:\n",
        "            list: Indices of context words\n",
        "        \"\"\"\n",
        "        contexts = []\n",
        "        center_word = self.data[idx]\n",
        "\n",
        "        # For each position within the window\n",
        "        for offset in range(-self.window_size, self.window_size + 1):\n",
        "            if offset == 0:  # Skip the center word itself\n",
        "                continue\n",
        "\n",
        "            context_idx = idx + offset\n",
        "            if 0 <= context_idx < len(self.data):\n",
        "                context_word = self.data[context_idx]\n",
        "                contexts.append(context_word)\n",
        "\n",
        "        return contexts\n",
        "\n",
        "    def sample_negative(self, positive_samples):\n",
        "        \"\"\"\n",
        "        Sample negative word indices\n",
        "\n",
        "        Args:\n",
        "            positive_samples (list): Word indices to exclude from sampling\n",
        "\n",
        "        Returns:\n",
        "            list: Negative samples\n",
        "        \"\"\"\n",
        "        # Using unigram distribution raised to 3/4 power as in the paper\n",
        "        # Here we simplify by sampling based on word frequencies directly\n",
        "        neg_samples = []\n",
        "        word_freq = np.array([self.word_counts[self.idx2word[i]] for i in range(self.vocab_size)])\n",
        "        word_freq = word_freq ** 0.75\n",
        "        word_prob = word_freq / np.sum(word_freq)\n",
        "\n",
        "        while len(neg_samples) < self.negative_samples:\n",
        "            sample = np.random.choice(self.vocab_size, p=word_prob)\n",
        "            if sample not in positive_samples:\n",
        "                neg_samples.append(sample)\n",
        "\n",
        "        return neg_samples\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Numerically stable sigmoid function\"\"\"\n",
        "        return np.where(x >= 0,\n",
        "                       1 / (1 + np.exp(-x)),\n",
        "                       np.exp(x) / (1 + np.exp(x)))\n",
        "\n",
        "    def train(self, epochs=5, batch_size=512, print_every=1000):\n",
        "        \"\"\"\n",
        "        Train the Skip-Gram model\n",
        "\n",
        "        Args:\n",
        "            epochs (int): Number of epochs to train\n",
        "            batch_size (int): Mini-batch size\n",
        "            print_every (int): How often to print progress\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'data'):\n",
        "            raise ValueError(\"No dataset loaded. Call build_vocab() first.\")\n",
        "\n",
        "        print(f\"Training Skip-Gram with {self.vocab_size} words, {self.embedding_dim} dimensions\")\n",
        "        print(f\"Window size: {self.window_size}, Negative samples: {self.negative_samples}\")\n",
        "\n",
        "        # Total number of word-context pairs\n",
        "        n_examples = len(self.data)\n",
        "        steps_per_epoch = n_examples // batch_size\n",
        "        total_steps = epochs * steps_per_epoch\n",
        "\n",
        "        # Training loop\n",
        "        loss_history = []\n",
        "        step = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "\n",
        "            # Create list of word indices to process\n",
        "            indices = list(range(len(self.data)))\n",
        "            random.shuffle(indices)\n",
        "\n",
        "            for batch_start in range(0, len(indices), batch_size):\n",
        "                batch_indices = indices[batch_start:batch_start + batch_size]\n",
        "                batch_loss = 0\n",
        "\n",
        "                # Dynamic learning rate decay\n",
        "                current_lr = self.initial_lr * (1 - step / total_steps)\n",
        "                current_lr = max(current_lr, self.initial_lr * 0.0001)  # Minimum learning rate\n",
        "\n",
        "                # Process each center word in the batch\n",
        "                for i in batch_indices:\n",
        "                    center_word_idx = self.data[i]\n",
        "                    # Apply subsampling - randomly skip frequent words\n",
        "                    if random.random() < self.discard_probs.get(self.idx2word[center_word_idx], 0):\n",
        "                        continue\n",
        "\n",
        "                    context_indices = self.get_target_contexts(i)\n",
        "\n",
        "                    # For each context word, perform update\n",
        "                    for context_idx in context_indices:\n",
        "                        # Positive sample (center word and context word)\n",
        "                        positive_sample = [center_word_idx, context_idx]\n",
        "\n",
        "                        # Get negative samples\n",
        "                        negative_samples = self.sample_negative(positive_sample)\n",
        "\n",
        "                        # Update for positive sample\n",
        "                        h = self.W_input[center_word_idx]  # Center word vector\n",
        "                        u = self.W_output[context_idx]     # Context word vector\n",
        "\n",
        "                        # Forward pass - positive sample\n",
        "                        score = np.dot(h, u)\n",
        "                        sigmoid_score = self.sigmoid(score)\n",
        "                        pos_loss = -np.log(sigmoid_score + 1e-10)\n",
        "\n",
        "                        # Gradients for positive sample\n",
        "                        grad_out = (sigmoid_score - 1) * h\n",
        "                        grad_in = (sigmoid_score - 1) * u\n",
        "\n",
        "                        # Update vectors for positive sample\n",
        "                        self.W_output[context_idx] -= current_lr * grad_out\n",
        "\n",
        "                        # Negative samples\n",
        "                        neg_loss = 0\n",
        "                        for neg_idx in negative_samples:\n",
        "                            neg_vec = self.W_output[neg_idx]\n",
        "                            neg_score = np.dot(h, neg_vec)\n",
        "                            neg_sigmoid = self.sigmoid(neg_score)\n",
        "                            neg_loss += -np.log(1 - neg_sigmoid + 1e-10)\n",
        "\n",
        "                            # Gradients for negative sample\n",
        "                            neg_grad_out = neg_sigmoid * h\n",
        "                            neg_grad_in = neg_sigmoid * neg_vec\n",
        "\n",
        "                            # Update vectors for negative sample\n",
        "                            self.W_output[neg_idx] -= current_lr * neg_grad_out\n",
        "                            grad_in += neg_grad_in\n",
        "\n",
        "                        # Update input vector once with accumulated gradients\n",
        "                        self.W_input[center_word_idx] -= current_lr * grad_in\n",
        "\n",
        "                        # Add loss\n",
        "                        batch_loss += (pos_loss + neg_loss)\n",
        "\n",
        "                # Normalize batch loss\n",
        "                if len(batch_indices) > 0:\n",
        "                    batch_loss /= len(batch_indices)\n",
        "                    epoch_loss += batch_loss\n",
        "\n",
        "                step += 1\n",
        "\n",
        "                # Print progress\n",
        "                if step % print_every == 0:\n",
        "                    elapsed = time.time() - start_time\n",
        "                    print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_start//batch_size+1}/{steps_per_epoch}, \"\n",
        "                          f\"Loss: {batch_loss:.4f}, Time: {elapsed:.2f}s\")\n",
        "                    loss_history.append(batch_loss)\n",
        "\n",
        "            # Epoch complete\n",
        "            avg_epoch_loss = epoch_loss / steps_per_epoch\n",
        "            print(f\"Epoch {epoch+1}/{epochs} complete. Average loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "        # Normalize embeddings\n",
        "        self.normalize_embeddings()\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def normalize_embeddings(self):\n",
        "        \"\"\"L2 normalize word vectors\"\"\"\n",
        "        norms = np.sqrt(np.sum(self.W_input ** 2, axis=1, keepdims=True))\n",
        "        self.W_input = self.W_input / (norms + 1e-10)\n",
        "\n",
        "    def get_word_vector(self, word):\n",
        "        \"\"\"Get embedding vector for a word\"\"\"\n",
        "        if word in self.word2idx:\n",
        "            return self.W_input[self.word2idx[word]]\n",
        "        else:\n",
        "            print(f\"Word '{word}' not in vocabulary\")\n",
        "            return None\n",
        "\n",
        "    def most_similar(self, word, n=10):\n",
        "        \"\"\"\n",
        "        Find n most similar words to a given word\n",
        "\n",
        "        Args:\n",
        "            word (str): Query word\n",
        "            n (int): Number of similar words to return\n",
        "\n",
        "        Returns:\n",
        "            list: Tuples of (word, similarity score)\n",
        "        \"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            print(f\"Word '{word}' not in vocabulary\")\n",
        "            return []\n",
        "\n",
        "        query_vec = self.get_word_vector(word)\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        similarities = cosine_similarity([query_vec], self.W_input)[0]\n",
        "\n",
        "        # Get top n similar words (excluding the query word itself)\n",
        "        most_similar = []\n",
        "        sorted_ids = np.argsort(similarities)[::-1]\n",
        "\n",
        "        for idx in sorted_ids:\n",
        "            if self.idx2word[idx] != word:\n",
        "                most_similar.append((self.idx2word[idx], similarities[idx]))\n",
        "                if len(most_similar) >= n:\n",
        "                    break\n",
        "\n",
        "        return most_similar\n",
        "\n",
        "    def analogy(self, word1, word2, word3, n=5):\n",
        "        \"\"\"\n",
        "        Solve word analogies like \"king - man + woman = queen\"\n",
        "\n",
        "        Args:\n",
        "            word1 (str): First word in analogy (e.g., \"king\")\n",
        "            word2 (str): Second word in analogy (e.g., \"man\")\n",
        "            word3 (str): Third word in analogy (e.g., \"woman\")\n",
        "            n (int): Number of results to return\n",
        "\n",
        "        Returns:\n",
        "            list: Tuples of (word, similarity score)\n",
        "        \"\"\"\n",
        "        for word in [word1, word2, word3]:\n",
        "            if word not in self.word2idx:\n",
        "                print(f\"Word '{word}' not in vocabulary\")\n",
        "                return []\n",
        "\n",
        "        # Compute target vector: vec(word1) - vec(word2) + vec(word3)\n",
        "        vec1 = self.get_word_vector(word1)\n",
        "        vec2 = self.get_word_vector(word2)\n",
        "        vec3 = self.get_word_vector(word3)\n",
        "\n",
        "        target_vec = vec1 - vec2 + vec3\n",
        "\n",
        "        # Normalize target vector\n",
        "        target_vec = target_vec / np.sqrt(np.sum(target_vec ** 2))\n",
        "\n",
        "        # Compute cosine similarities\n",
        "        similarities = cosine_similarity([target_vec], self.W_input)[0]\n",
        "\n",
        "        # Get top results (excluding the input words)\n",
        "        results = []\n",
        "        sorted_ids = np.argsort(similarities)[::-1]\n",
        "\n",
        "        for idx in sorted_ids:\n",
        "            word = self.idx2word[idx]\n",
        "            if word not in [word1, word2, word3]:\n",
        "                results.append((word, similarities[idx]))\n",
        "                if len(results) >= n:\n",
        "                    break\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate_similarity(self, similarity_dataset):\n",
        "        \"\"\"\n",
        "        Evaluate the model on a word similarity dataset\n",
        "\n",
        "        Args:\n",
        "            similarity_dataset (list): List of (word1, word2, similarity_score) tuples\n",
        "\n",
        "        Returns:\n",
        "            float: Spearman correlation between model similarities and human ratings\n",
        "        \"\"\"\n",
        "        model_scores = []\n",
        "        human_scores = []\n",
        "        oov_words = set()\n",
        "\n",
        "        for word1, word2, score in similarity_dataset:\n",
        "            if word1 not in self.word2idx:\n",
        "                oov_words.add(word1)\n",
        "                continue\n",
        "            if word2 not in self.word2idx:\n",
        "                oov_words.add(word2)\n",
        "                continue\n",
        "\n",
        "            vec1 = self.get_word_vector(word1)\n",
        "            vec2 = self.get_word_vector(word2)\n",
        "\n",
        "            sim = cosine_similarity([vec1], [vec2])[0][0]\n",
        "            model_scores.append(sim)\n",
        "            human_scores.append(float(score))\n",
        "\n",
        "        if oov_words:\n",
        "            print(f\"Warning: {len(oov_words)} words not in vocabulary: {', '.join(list(oov_words)[:10])}\")\n",
        "            if len(oov_words) > 10:\n",
        "                print(f\"...and {len(oov_words) - 10} more\")\n",
        "\n",
        "        if len(model_scores) < 2:\n",
        "            print(\"Not enough pairs to evaluate\")\n",
        "            return 0.0\n",
        "\n",
        "        correlation, p_value = spearmanr(human_scores, model_scores)\n",
        "        print(f\"Spearman correlation: {correlation:.4f} (p-value: {p_value:.4f})\")\n",
        "        print(f\"Evaluated on {len(model_scores)} word pairs\")\n",
        "\n",
        "        return correlation\n",
        "\n",
        "    def evaluate_analogies(self, analogy_dataset):\n",
        "        \"\"\"\n",
        "        Evaluate the model on word analogy tasks\n",
        "\n",
        "        Args:\n",
        "            analogy_dataset (list): List of (a, b, c, d) tuples representing a:b :: c:d\n",
        "\n",
        "        Returns:\n",
        "            float: Accuracy of the model on analogy task\n",
        "        \"\"\"\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        oov_analogies = 0\n",
        "\n",
        "        for a, b, c, d in analogy_dataset:\n",
        "            if any(word not in self.word2idx for word in [a, b, c, d]):\n",
        "                oov_analogies += 1\n",
        "                continue\n",
        "\n",
        "            # Get top result from analogy task\n",
        "            results = self.analogy(a, b, c, n=1)\n",
        "            if not results:\n",
        "                continue\n",
        "\n",
        "            predicted_word, _ = results[0]\n",
        "\n",
        "            if predicted_word == d:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "        if total == 0:\n",
        "            print(\"No valid analogies found for evaluation\")\n",
        "            return 0.0\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f\"Analogy accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
        "        print(f\"Out-of-vocabulary analogies: {oov_analogies}\")\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def visualize_embeddings(self, words, method='tsne'):\n",
        "        \"\"\"\n",
        "        Visualize word embeddings in 2D space\n",
        "\n",
        "        Args:\n",
        "            words (list): List of words to visualize\n",
        "            method (str): Dimensionality reduction method ('tsne' or 'pca')\n",
        "        \"\"\"\n",
        "        if method == 'tsne':\n",
        "            from sklearn.manifold import TSNE\n",
        "            reducer = TSNE(n_components=2, random_state=42)\n",
        "        elif method == 'pca':\n",
        "            from sklearn.decomposition import PCA\n",
        "            reducer = PCA(n_components=2, random_state=42)\n",
        "        else:\n",
        "            raise ValueError(\"Method must be 'tsne' or 'pca'\")\n",
        "\n",
        "        # Filter words that are in vocabulary\n",
        "        valid_words = [w for w in words if w in self.word2idx]\n",
        "        if len(valid_words) == 0:\n",
        "            print(\"None of the provided words are in vocabulary\")\n",
        "            return\n",
        "\n",
        "        # Get embeddings for valid words\n",
        "        word_vectors = np.array([self.get_word_vector(w) for w in valid_words])\n",
        "\n",
        "        # Reduce dimensions\n",
        "        embedding_2d = reducer.fit_transform(word_vectors)\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], alpha=0.7)\n",
        "\n",
        "        # Add labels\n",
        "        for i, word in enumerate(valid_words):\n",
        "            plt.annotate(word, (embedding_2d[i, 0], embedding_2d[i, 1]),\n",
        "                         fontsize=11, alpha=0.8)\n",
        "\n",
        "        plt.title(f\"Word Embeddings visualized using {method.upper()}\")\n",
        "        plt.grid(True, alpha=0.4)\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save_embeddings(self, filename):\n",
        "        \"\"\"\n",
        "        Save word embeddings to a text file (word2vec format)\n",
        "\n",
        "        Args:\n",
        "            filename (str): Path to save the embeddings\n",
        "        \"\"\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"{self.vocab_size} {self.embedding_dim}\\n\")\n",
        "            for word, idx in self.word2idx.items():\n",
        "                vector_str = ' '.join(str(val) for val in self.W_input[idx])\n",
        "                f.write(f\"{word} {vector_str}\\n\")\n",
        "\n",
        "        print(f\"Embeddings saved to {filename}\")\n",
        "\n",
        "    def load_embeddings(self, filename):\n",
        "        \"\"\"\n",
        "        Load word embeddings from a text file (word2vec format)\n",
        "\n",
        "        Args:\n",
        "            filename (str): Path to the embeddings file\n",
        "        \"\"\"\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            header = f.readline().split()\n",
        "            vocab_size, embedding_dim = int(header[0]), int(header[1])\n",
        "\n",
        "            self.embedding_dim = embedding_dim\n",
        "            self.W_input = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "            for i, line in enumerate(f):\n",
        "                parts = line.rstrip().split(' ')\n",
        "                word = parts[0]\n",
        "                vector = np.array([float(val) for val in parts[1:]])\n",
        "\n",
        "                self.word2idx[word] = i\n",
        "                self.idx2word[i] = word\n",
        "                self.W_input[i] = vector\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.W_output = np.zeros_like(self.W_input)  # Initialize output matrix\n",
        "\n",
        "        print(f\"Loaded embeddings for {self.vocab_size} words with dimension {self.embedding_dim}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bEEVEr_0QhCZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Sample text for demonstration\n",
        "    sample_text = \"\"\"\n",
        "    Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence\n",
        "    concerned with the interactions between computers and human language, in particular how to program computers\n",
        "    to process and analyze large amounts of natural language data. The goal is a computer capable of understanding\n",
        "    the contents of documents, including the contextual nuances of the language within them. The technology can then\n",
        "    accurately extract information and insights contained in the documents as well as categorize and organize the\n",
        "    documents themselves.\n",
        "\n",
        "    Challenges in natural language processing frequently involve speech recognition, natural language understanding,\n",
        "    and natural language generation. Modern NLP algorithms are based on machine learning, especially statistical\n",
        "    machine learning and deep learning approaches. Deep learning models have achieved state-of-the-art results\n",
        "    in many natural language tasks. These tasks include machine translation, text classification, discourse analysis,\n",
        "    question answering and many more.\n",
        "\n",
        "    Word embeddings are a type of word representation that allows words with similar meaning to have a similar\n",
        "    representation. They are a distributed representation for text that is perhaps one of the key breakthroughs\n",
        "    for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
        "\n",
        "    The skip-gram model is an efficient method for learning high-quality distributed vector representations that\n",
        "    capture a large number of precise syntactic and semantic word relationships. It represents each word in a\n",
        "    lower-dimensional vector space by considering the context in which the word appears.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create and train model\n",
        "    model = SkipGram(sample_text, embedding_dim=50, window_size=2, min_count=2)\n",
        "    model.train(epochs=10, batch_size=16, print_every=50)\n",
        "\n",
        "    # Display similar words\n",
        "    print(\"\\nMost similar words to 'language':\")\n",
        "    similar_words = model.most_similar('language', n=5)\n",
        "    for word, score in similar_words:\n",
        "        print(f\"{word}: {score:.4f}\")\n",
        "\n",
        "    # Try a word analogy\n",
        "    print(\"\\nWord analogy: language - natural + artificial = ?\")\n",
        "    analogy_results = model.analogy('language', 'natural', 'artificial', n=3)\n",
        "    for word, score in analogy_results:\n",
        "        print(f\"{word}: {score:.4f}\")\n",
        "\n",
        "    # Visualize some word embeddings\n",
        "    words_to_visualize = ['natural', 'language', 'processing', 'computer', 'artificial',\n",
        "                         'intelligence', 'machine', 'learning', 'model', 'vector', 'word']\n",
        "    model.visualize_embeddings(words_to_visualize, method='pca')\n",
        "\n",
        "    # Sample evaluation datasets\n",
        "    similarity_dataset = [\n",
        "        ('natural', 'language', 0.8),\n",
        "        ('machine', 'learning', 0.9),\n",
        "        ('artificial', 'intelligence', 0.85),\n",
        "        ('computer', 'machine', 0.7),\n",
        "        ('language', 'word', 0.6)\n",
        "    ]\n",
        "\n",
        "    analogy_dataset = [\n",
        "        ('natural', 'language', 'artificial', 'intelligence'),\n",
        "        ('language', 'processing', 'machine', 'learning'),\n",
        "        ('word', 'vector', 'deep', 'learning')\n",
        "    ]\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"\\nEvaluating model on word similarity task:\")\n",
        "    model.evaluate_similarity(similarity_dataset)\n",
        "\n",
        "    print(\"\\nEvaluating model on word analogy task:\")\n",
        "    model.evaluate_analogies(analogy_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9LqhQuuCQhFg",
        "outputId": "11f1924b-8ad1-4dae-d217-50def7dbd753"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 34\n",
            "Total words in dataset after filtering: 134\n",
            "Training Skip-Gram with 34 words, 50 dimensions\n",
            "Window size: 2, Negative samples: 5\n",
            "Epoch 1/10 complete. Average loss: 0.7798\n",
            "Epoch 2/10 complete. Average loss: 0.1949\n",
            "Epoch 3/10 complete. Average loss: 0.1300\n",
            "Epoch 4/10 complete. Average loss: 0.5199\n",
            "Epoch 5/10 complete. Average loss: 0.5199\n",
            "Epoch 6/10, Batch 5/8, Loss: 1.0397, Time: 0.08s\n",
            "Epoch 6/10 complete. Average loss: 0.3899\n",
            "Epoch 7/10 complete. Average loss: 0.5199\n",
            "Epoch 8/10 complete. Average loss: 0.5199\n",
            "Epoch 9/10 complete. Average loss: 0.3899\n",
            "Epoch 10/10 complete. Average loss: 0.1300\n",
            "\n",
            "Most similar words to 'language':\n",
            "tasks: 0.3094\n",
            "representation: 0.2761\n",
            "understanding: 0.2712\n",
            "is: 0.2659\n",
            "the: 0.1735\n",
            "\n",
            "Word analogy: language - natural + artificial = ?\n",
            "Word 'artificial' not in vocabulary\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating model on word similarity task:\n",
            "Warning: 1 words not in vocabulary: artificial\n",
            "Spearman correlation: 0.8000 (p-value: 0.2000)\n",
            "Evaluated on 4 word pairs\n",
            "\n",
            "Evaluating model on word analogy task:\n",
            "Analogy accuracy: 0.0000 (0/2)\n",
            "Out-of-vocabulary analogies: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5sVhaABQhIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xnxoj-0ZQhLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RbKsAYDQhPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5DIJZeO3Qexh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source"
      ],
      "metadata": {
        "id": "lMEboYMTD2BP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://spotintelligence.com/2023/07/11/skip-gram-models-explained-how-to-create-embeddings-in-word2vec/"
      ],
      "metadata": {
        "id": "DICWp37pVXcG"
      }
    }
  ]
}