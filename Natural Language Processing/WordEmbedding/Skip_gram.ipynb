{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram"
      ],
      "metadata": {
        "id": "pUJitngFVXFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Skip-gram is a popular algorithm used in natural language processing (NLP), specifically in word embedding techniques. It is a method for learning word representations in a vector space, often used in the context of word2vec models.\n",
        "\n",
        "- The main idea behind skip-gram is to predict the context words (words surrounding a target word) given a target word. It treats the target word as input and tries to maximize the probability of predicting the context words within a specified window around the target word\n",
        "\n",
        "- The learned word embeddings capture semantic relationships between words, allowing for efficient and meaningful word representations in downstream NLP applications."
      ],
      "metadata": {
        "id": "j6Zvd4ddVXNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word embeddings with skip-grams"
      ],
      "metadata": {
        "id": "3OWXd7ZlVXPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2vec is a popular set of algorithms for learning word embeddings from large text corpora. It consists of two primary models: **the skip-gram and continuous bag-of-words (CBOW) models.**\n",
        "\n"
      ],
      "metadata": {
        "id": "WpvCyy7uVXRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram Implementation"
      ],
      "metadata": {
        "id": "KA7x9LH1VXT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Data Preparation:**\n",
        "\n",
        "- The first step is to prepare your training data, typically a large text corpus. The text is **tokenized** into individual words and optionally preprocessed by **removing punctuation, converting to lowercase, etc**\n",
        "\n",
        "**2.Context-Target Pairs:**\n",
        "\n",
        "- The skip-gram model aims to **predict the surrounding context words for each word in the training data**.\n",
        "- The **context is defined by a window size**, which determines the number of words **before and after the target word** that are considered context words.\n",
        "- Consider an example sentence: “I love to eat pizza.”\n",
        "\n",
        "If we set the window size to 2, the context-target pairs for the word “love” would be:\n",
        "\n",
        "        - Context: [I, to, eat]\n",
        "        - Target: love\n",
        "        - Similarly, we create context-target pairs for all the words in the training data.\n",
        "\n",
        "**3.Neural Network Architecture:**\n",
        "\n",
        "- The skip-gram model comprises a **single hidden neural network with a projection layer.**\n",
        "- The **input** layer represents the **target** word, and the **projection** layer represents the **word embeddings or vector representations**.\n",
        "The projection layer has weights that correspond to each word in the vocabulary. Each weight vector represents the word embedding for that particular word.\n",
        "**The size of the projection layer (the dimensionality of the word embeddings) is a hyperparameter that needs to be specified before training.**\n"
      ],
      "metadata": {
        "id": "Us_mTP5JVXVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Training:**\n",
        "\n",
        "- The objective of training the skip-gram model is **to maximize the probability of correctly predicting the context words given the target word**.\n",
        "- This is typically done using **stochastic gradient descent (SGD) or other optimization algorithms**.\n",
        "- The training process involves **updating the weights of the projection layer to minimize the loss between the predicted and actual context words.**\n",
        "- The model learns to adjust the word embeddings such that similar words have similar vector representations in the embedding space.\n",
        "\n",
        "**5.Word Embeddings**\n",
        "- Once the skip-gram model is trained, the **word embeddings are extracted from the projection layer**.\n",
        "- These word embeddings capture the **semantic relationships between words in the training data**.\n",
        "- The **dimensionality of the word embeddings, determined by the size of the projection layer**, can be chosen based on the desired trade-off between computational efficiency and semantic expressiveness.\n",
        "- The word embeddings can be used as input features for various downstream NLP tasks or for measuring **word similarity, clustering words, and other linguistic analyses.**"
      ],
      "metadata": {
        "id": "-iYqbnvVVXXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip-gram models are successful in capturing semantic relationships.**\n",
        "\n",
        "The skip-gram model in the word2vec framework has been successful in capturing semantic relationships between words, such as word analogies\n",
        "\n",
        "**(e.g., “king” – “man” + “woman” ≈ “queen”)**. It has demonstrated its usefulness in various NLP applications, including **language modelling, sentiment analysis, machine translation, and information retrieval**.\n",
        "\n"
      ],
      "metadata": {
        "id": "1rlKzFYOVXZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages"
      ],
      "metadata": {
        "id": "79iRLXYzVXaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Captures Semantic Relationships**: It learns word embeddings that encode similar meanings and associations, allowing for tasks like word analogies and similarity calculations.\n",
        "\n",
        "- **Handles Rare Words**: The skip-gram model performs well even with rare words or words with limited occurrences in the training data. It can generate meaningful representations for such words by leveraging the context in which they appear\n",
        "\n",
        "- **Contextual Flexibility**: The skip-gram model allows for flexible context definitions by using a window around each target word. This flexibility captures local and global word associations, resulting in richer semantic representations.\n",
        "\n",
        "- **Scalability**: The skip-gram model can be trained efficiently on large-scale datasets due to its simplicity and parallelization potential. It can process vast amounts of text data to generate high-quality word embeddings"
      ],
      "metadata": {
        "id": "a5nE82bJD1of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disadvantages"
      ],
      "metadata": {
        "id": "elAibxAMD1rH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Increased Training Time:** Training the skip-gram model can be computationally expensive, especially when dealing with larger vocabularies and higher-dimensional embeddings. The model requires iterating over a large amount of training data and updating numerous parameters, which can slow down the training process.\n",
        "\n",
        "- **Higher Memory Requirements**: The skip-gram model tends to have higher memory requirements than the continuous bag-of-words (CBOW) model. This is because the skip-gram model needs to store a separate vector representation for each word in the vocabulary, which can become memory-intensive for larger vocabularies\n",
        "\n",
        "- **Limited Contextual Information**: The skip-gram model considers only local word contexts within the defined window. While this approach is useful for many NLP tasks, it may not capture long-range dependencies or more complex contextual information beyond the window size.\n",
        "\n",
        "- **Data Efficiency**: The skip-gram model may require much training data to achieve robust word embeddings, particularly for low-frequency or rare words. If the training data is limited, the model may struggle to accurately capture the full semantic space.\n",
        "\n",
        "- **Lack of Document-Level Information**: The skip-gram model treats each sentence or text snippet as an independent training example. It doesn’t consider document-level or global information, potentially limiting its ability to capture higher-level semantic relationships across multiple sentences or documents.\n"
      ],
      "metadata": {
        "id": "a56Jc_JND1to"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "n0fT1MJvD1wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Intrinsic Evaluation**\n",
        "\n",
        "The intrinsic evaluation evaluates word embeddings based on specific linguistic or semantic tasks.\n",
        "- **Word Similarity**: Measure the cosine similarity or other distance metrics between word embeddings and compare them to human-annotated similarity scores.\n",
        "- **Word Analogies**: Test the ability of word embeddings to solve analogy tasks, such as “king – man + woman = queen,” by measuring the cosine similarity between the resulting vector and the expected vector.\n",
        "- **Word Clustering**: Cluster words based on their embeddings and assess the clustering quality using external evaluation metrics like purity or entropy.\n",
        "- **Part-of-Speech Tagging**: Utilize word embeddings as features in a part-of-speech tagging task and compare the performance to other approaches.\n",
        "\n",
        "**2.Extrinsic Evaluation:**\n",
        "\n",
        "Extrinsic evaluation involves evaluating word embeddings on downstream NLP tasks that utilize word representations as input features.\n",
        "- **Sentiment Analysis**: Use word embeddings as features for sentiment classification and compare the performance with other feature representations.\n",
        "- **Named Entity Recognition**: Assess the impact of word embeddings on named entity recognition tasks by incorporating them as input features and measuring the performance.\n",
        "- **Machine Translation**: Use word embeddings in neural machine translation models and evaluate the translation quality compared to other embedding techniques.\n",
        "\n",
        "**3.Word Embedding Analogies**\n",
        "\n",
        "- Evaluate the quality of word embeddings by manually inspecting and interpreting the relationships between words. Look for analogy patterns and assess if they hold based on semantic or syntactic properties.\n",
        "\n",
        "**4.Word Embedding Visualization**\n",
        "\n",
        "- Visualize word embeddings in a lower-dimensional space (e.g., 2D or 3D) using techniques like **t-SNE or PCA**. Examine the spatial relationships between words and inspect clusters or semantic groupings.\n",
        "\n",
        "**5.Benchmark Datasets**\n",
        "\n",
        "- Use benchmark datasets such as **WordSim-353, SimLex-999, or Word2Vec-Google** analogies to evaluate word embeddings. These datasets provide standardized evaluation metrics for comparing different embedding techniques.\n"
      ],
      "metadata": {
        "id": "prJJNLdHD1yQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternatives to skip-grams"
      ],
      "metadata": {
        "id": "IR2oTaf5D10v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Continuous Bag-of-Words (CBOW)**\n",
        "\n",
        "- CBOW is another model in the Word2Vec framework that aims to **predict a target word given its surrounding context words.**\n",
        "- In contrast, to skip grams, CBOW predicts the target word by summing up the embeddings of the context words. It treats the context words as the input and the target word as the output.\n",
        "- CBOW is computationally efficient and useful when less training data or frequent words dominate the training set.\n",
        "\n",
        "**2.GloVe (Global Vectors for Word Representation)**\n",
        "\n",
        "- GloVe is a word embedding technique that combines the advantages of **global matrix factorization and local context windows.**\n",
        "- It constructs a **co-occurrence matrix based on word-word co-occurrence statistics** from the corpus.\n",
        "- GloVe then **factorizes this matrix to learn word embeddings that capture local and global word relationships**.\n",
        "- GloVe embeddings are often pre-trained on large corpora and are widely used for various NLP tasks.\n",
        "\n",
        "**3.FastText**\n",
        "\n",
        "- FastText is an extension of Word2Vec that represents words as **bags of character n-grams.**\n",
        "- Instead of treating words as atomic units, FastText **breaks words into character-level n-grams (e.g., “apple” is represented as “ap,” “app,” “ppl,” “plan,” “le”).**\n",
        "- The model learns embeddings for the n-grams and combines them to form word representations. This approach is particularly useful for **handling out-of-vocabulary words and capturing morphological information**.\n",
        "\n",
        "**4.ELMo (Embeddings from Language Models)**\n",
        "\n",
        "- ELMo is a **deep contextualized word representation model considering word meanings in different contexts.**\n",
        "- It uses a **bidirectional language model to generate word embeddings that capture context-dependent information**.\n",
        "- ELMo embeddings are effective in various NLP tasks, as they capture nuances of word meaning and semantic relationships in different linguistic contexts.\n",
        "\n",
        "**5.Transformer-based Models**\n",
        "\n",
        "- Transformer-based models like **BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer)** have gained popularity for learning contextualized word embeddings.\n",
        "- These models employ **self-attention mechanisms** to encode words in context, enabling more accurate and context-dependent word representations.\n"
      ],
      "metadata": {
        "id": "Jjrmf0BaD13x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications"
      ],
      "metadata": {
        "id": "PAbjNTgmD18J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mLWlP_SgM_wl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Word Embeddings**\n",
        "\n",
        "- Skip-gram Word2Vec models learn word embeddings, dense vector representations of words in a continuous vector space. These embeddings can be utilized as features in downstream NLP tasks to enhance their performance. For example:\n",
        "\n",
        "  - **Sentiment Analysis**: Word embeddings can be used as input features in sentiment analysis models to capture semantic information and improve sentiment classification accuracy.\n",
        "  - **Named Entity Recognition**: Word embeddings can serve as input features for named entity recognition models, helping to identify and classify named entities based on their context.\n",
        "\n",
        "**2.Text Classification**\n",
        "\n",
        "Skip-gram Word2Vec embeddings can be used in classification tasks to represent **text as feature vectors**. The models can capture **semantic information and improve classification accuracy by converting words into embeddings**.\n",
        "\n",
        "**3.Language Modeling**\n",
        "\n",
        "Skip-gram Word2Vec models can improve language modelling tasks by predicting the next word in a sequence or generating coherent and contextually relevant text\n",
        "\n",
        "**4.Information Retrieval**\n",
        "\n",
        "Word embeddings learned by skip-gram Word2Vec models can be used to enhance information retrieval systems. By representing words as vectors in a semantic space, similarity measures like **cosine similarity** can be employed to find similar or related words, improving search results and query expansion.\n",
        "\n",
        "**5.Machine Translation**\n",
        "\n",
        "Skip-gram Word2Vec embeddings can be beneficial in machine translation tasks. They can help capture **semantic relationships between words in different languages, improving the translation quality and handling lexical and semantic variations.**\n",
        "\n",
        "**6.Word Similarity and Analogies**\n",
        "\n",
        "Skip-gram Word2Vec models can measure word similarity by calculating cosine similarity between word embeddings. They can also be employed to solve word analogy tasks (e.g., “king – man + woman = queen”) by finding the most similar word vectors based on their semantic relationships.\n",
        "\n",
        "**7.Pretraining for Transfer Learning**\n",
        "\n",
        "Skip-gram Word2Vec models can be pre-trained on large corpora and used as a starting point for transfer learning in various NLP tasks. By leveraging the learned word embeddings, models can benefit from the semantic knowledge captured by skip-gram Word2Vec.\n",
        "\n"
      ],
      "metadata": {
        "id": "RgXV-CnBM_0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source"
      ],
      "metadata": {
        "id": "lMEboYMTD2BP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://spotintelligence.com/2023/07/11/skip-gram-models-explained-how-to-create-embeddings-in-word2vec/"
      ],
      "metadata": {
        "id": "DICWp37pVXcG"
      }
    }
  ]
}