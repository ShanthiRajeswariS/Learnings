{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CountVectorizer in NLP"
      ],
      "metadata": {
        "id": "_76Q-MoFjrBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CountVectorizer is a text **preprocessing technique** commonly used in natural language processing (NLP) tasks for **converting a collection of text documents into a numerical representation**.\n",
        "- It is part of the scikit-learn library, a popular machine learning library in Python.\n",
        "- CountVectorizer operates **by tokenizing the text data and counting the occurrences of each token**.\n",
        "- It then creates a **matrix where the rows represent the documents, and the columns represent the tokens**.\n",
        "- The cell values indicate the frequency of each token in each document. This matrix is known as the **“document-term matrix.”**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o5Hguy8rjrEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation using Scikit-learn"
      ],
      "metadata": {
        "id": "ti8xsNNmjrG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the documents and transform the documents into a document-term matrix\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (tokens)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the feature names\n",
        "print(feature_names)\n",
        "\n",
        "# Print the document-term matrix\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNwlv_-Nkvek",
        "outputId": "b608cd9a-48c6-4c59-ff44-a51dbdf10e4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CountVectorizer offers various parameters and options to control its behaviour, such as **specifying the minimum document frequency for a token to be included, removing stop words, and using n-grams instead of single tokens**.\n",
        "- These options can be explored in the scikit-learn documentation for further customization based on specific needs."
      ],
      "metadata": {
        "id": "_F2eNQ-CjrJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages"
      ],
      "metadata": {
        "id": "fISnYjFojrMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Simplicity**: CountVectorizer is easy to use and understand. It has specific parameters and requires minimal configuration to get started with text preprocessing.\n",
        "\n",
        "- **Speed and Efficiency**: CountVectorizer is computationally efficient and can handle large text datasets with many documents. It utilizes sparse matrix representations to save memory and processing time, especially when dealing with high-dimensional data.\n",
        "\n",
        "- **Versatility**: CountVectorizer allows for flexible tokenization options, including handling n-grams (consecutive sequences of words) and custom token patterns. It also provides opportunities for filtering stop words and controlling the vocabulary size.\n",
        "\n",
        "- **Interpretable Results**: The resulting document-term matrix from CountVectorizer provides interpretable results. Each cell in the matrix represents the count or frequency of a token in a specific document, allowing for straightforward analysis and exploration.\n"
      ],
      "metadata": {
        "id": "2zZ8ioSIjrO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disadvantages"
      ],
      "metadata": {
        "id": "JWXp6w4pjrRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Ignores Semantic Information**: It treats each token as a separate entity and does not capture semantic relationships between words. It does not consider the context or meaning of words, which might limit its effectiveness in tasks that require an understanding of word semantics.\n",
        "\n",
        "- **Bias towards Frequent Words**: It assigns higher importance to words that frequently appear in documents. Consequently, common words like “the,” “and,” or “is” may dominate the feature space while potentially ignoring rarer but more meaningful words.\n",
        "\n",
        "- **Lack of Normalization**: It does not consider document length, meaning longer documents may have higher token counts than shorter documents, even if they discuss the same topics. This lack of normalization might affect specific analyses and algorithms that rely on document length.\n",
        "\n",
        "- **Limited Information**: It only captures the frequency of tokens within documents. It does not consider the order or sequence of words, which may be relevant in specific text analysis tasks like sentiment analysis or language modelling.\n"
      ],
      "metadata": {
        "id": "yAnNvsCDjrT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternatives"
      ],
      "metadata": {
        "id": "Wocht3_rjrWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TfidfVectorizer"
      ],
      "metadata": {
        "id": "fb_RgpT2jrZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TfidfVectorizer stands for **“Term Frequency-Inverse Document Frequency Vectorizer**.”\n",
        "- It builds upon the concept of CountVectorizer but incorporates the TF-IDF weighting scheme.\n",
        "- TF-IDF is a numerical statistic that reflects the importance of a term (token) in a document within a larger corpus.\n",
        "\n",
        "- The TF-IDF value for a term in a document is calculated **by multiplying the term frequency (TF) and inverse document frequency (IDF) components:**\n",
        "\n",
        "  - **Term Frequency** (TF) represents the **frequency of a term in a document**. It is typically calculated as the count of the term in the document divided by the total number of terms in the document.\n",
        "  - **Inverse Document Frequency (IDF)** measures the **rarity of a term in the corpus**. It is **calculated as the logarithm of the total number of documents divided by the number of documents that contain the term**.\n",
        "\n",
        "- TfidfVectorizer tokenizes the text, counts the term frequencies, and applies the IDF transformation to obtain the TF-IDF representation. It creates a **matrix where the rows represent the documents, and the columns represent the tokens. The cell values indicate the TF-IDF weights of each token in each document.**\n",
        "\n"
      ],
      "metadata": {
        "id": "fZHfohcgjrbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CountVectorizer Vs TfidfVectorizer"
      ],
      "metadata": {
        "id": "7_ezhRCBqIQP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CountVectorizer**\n",
        "\n",
        "- CountVectorizer **converts a collection of text documents into a matrix where the rows represent the documents, and the columns represent the tokens** (words or n-grams).\n",
        "- It counts the occurrences of each token in each document, creating a **“document-term matrix”** with integer values representing the frequency of each token.\n",
        "- CountVectorizer **does not consider the importance of tokens**; it simply **counts the occurrences**.\n",
        "- It is helpful for tasks **where the frequency of tokens is essential, such as text classification or clustering based on word frequency.**\n",
        "- Countvectorizer is a **simple technique that counts the number of times a word occurs**\n",
        "\n",
        "**TfidfVectorizer**\n",
        "\n",
        "- TfidfVectorizer stands for “Term Frequency-Inverse Document Frequency.”\n",
        "Like CountVectorizer, it **converts text documents into a matrix representation**.\n",
        "- However, **TfidfVectorizer considers the frequency of tokens in each document and incorporates the inverse document frequency**.\n",
        "- The inverse document frequency component down weights the tokens that frequently appear across all documents, **giving more weight to rare tokens in the corpus**.\n",
        "- TfidfVectorizer computes a weight for each token in each document, considering both the term frequency (TF) and inverse document frequency (IDF) aspects.\n",
        "- It is helpful for tasks where the **frequency and rarity of tokens are essential, such as information retrieval, document ranking, or text summarization.**\n"
      ],
      "metadata": {
        "id": "BCOeI6xlqLKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "Nddak7sVrOGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_count = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "# TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Get the feature names (tokens)\n",
        "feature_names_count = count_vectorizer.get_feature_names_out()\n",
        "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the feature names\n",
        "print(\"CountVectorizer feature names:\", feature_names_count)\n",
        "print(\"TfidfVectorizer feature names:\", feature_names_tfidf)\n",
        "\n",
        "# Print the document-term matrices\n",
        "print(\"CountVectorizer document-term matrix:\")\n",
        "print(X_count.toarray())\n",
        "\n",
        "print(\"TfidfVectorizer document-term matrix:\")\n",
        "print(X_tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHvR2Z56rNqw",
        "outputId": "c4f3a995-860f-4f95-da8f-218b638c970c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CountVectorizer feature names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "TfidfVectorizer feature names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
            "CountVectorizer document-term matrix:\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n",
            "TfidfVectorizer document-term matrix:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative to CountVectorizer"
      ],
      "metadata": {
        "id": "FGr-s32MreHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **HashingVectorizer**: HashingVectorizer is a **memory-efficient** alternative to CountVectorizer and TfidfVectorizer. Instead of building and storing a vocabulary, it uses a **hashing function to convert tokens into numerical representations directly**. This approach avoids the need to keep the entire vocabulary in memory but can lead to potential collisions where different tokens might be hashed to the same value.\n",
        "\n",
        "- **Word2Vec**: Word2Vec is a word embedding technique representing words as **dense vectors in a continuous vector space**. It captures **semantic relationships between words by considering their context in large text corpora**. Word2Vec can be trained on large datasets, or pre-trained models can be used for transfer learning. It provides dense, low-dimensional representations that encode semantic information.\n",
        "\n",
        "- **GloVe: GloVe (Global Vectors for Word Representation)** is another word embedding technique that **learns word vectors by factorizing a word co-occurrence matrix. It combines the advantages of global context (capturing global word relationships) and local context (capturing local word relationships**). Pretrained GloVe word vectors are available for various languages and can be used for various NLP tasks.\n",
        "\n",
        "- **BERT (Bidirectional Encoder Representations from Transformers)**: BERT is a **state-of-the-art language model** that uses a transformer architecture to capture contextual information from text. It generates word embeddings that consider both **each word’s left and right context**. BERT can be fine-tuned on specific tasks or used as a feature extractor to obtain contextualized word representations."
      ],
      "metadata": {
        "id": "q4QN2Xeurhqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source"
      ],
      "metadata": {
        "id": "Q5ftNeCasHEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://spotintelligence.com/2023/05/17/countvectorizer/"
      ],
      "metadata": {
        "id": "6HB2AL17sInt"
      }
    }
  ]
}