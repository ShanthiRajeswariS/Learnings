{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost / Adaptive Boosting Ensemble Algorithm - No Feature Scaling with less hyperparameter tuning - Roughly uniform Distribution"
      ],
      "metadata": {
        "id": "MLFvFPRWHreO"
      },
      "id": "MLFvFPRWHreO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- machine learning algorithm that belongs to the **ensemble learning techniques**.\n",
        "\n",
        "- Ensemble learning involves **combining the predictions of multiple individual models to create a more accurate and robust final prediction**.\n",
        "\n",
        "- AdaBoost specifically focuses on **improving the performance of weak learners (individual models that are slightly better than random guessing) by sequentially training them on different subsets of the data and giving more weight to the misclassified samples.**\n",
        "\n",
        "- Instead of training a single, complex model, AdaBoost combines the predictions of many simple, \"weak\" learners to create a single, highly accurate \"strong\" learner."
      ],
      "metadata": {
        "id": "987fJdpMHrg8"
      },
      "id": "987fJdpMHrg8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Steps"
      ],
      "metadata": {
        "id": "BD9umEPYHrju"
      },
      "id": "BD9umEPYHrju"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Initialization of weights**: Each sample in the training dataset is **assigned an equal weight initially**. These weights determine the importance of each instance during the training process.\n",
        "\n",
        "2. **Training Weak Learners**: AdaBoost **starts by training a weak learner on the training data**. A weak learner is typically a simple model like a **decision tree with limited depth (a “stump”) or a linear classifier**. The weak learner’s performance might be just slightly better than random guessing.\n",
        "\n",
        "3. **Calculating Weak Learner Error/ Weighted Error**: After training the weak learner, it’s evaluated on the training data. The samples that the **weak learner misclassifies are assigned higher weights, making them more influential in subsequent iterations.**\n",
        "\n",
        "4. **Compute Alpha/ Calculating Weak Learner Weight(Influence)**: An alpha value is computed **based on the weighted error of the weak learner**. The alpha value indicates **how much trust should be given to the weak learner’s prediction**. A smaller weighted error leads to a higher alpha. A weight (or \"alpha\" value) is assigned to the current weak learner. This weight is inversely proportional to its error rate; weak learners that perform better (lower error) are given higher weights.\n",
        "\n",
        "5. **Update Weights**: The weights of the misclassified samples are updated, increasing their importance for the next iteration. Correctly classified samples retain their weights or may have their weights reduced.\n",
        "\n",
        "  a. Increase weights: Data points that were misclassified by the current weak learner have their weights increased. This makes them more \"important\" for the next weak learner, forcing it to focus on these challenging examples.\n",
        "\n",
        "  b. Decrease weights: Data points that were correctly classified have their weights decreased.\n",
        "\n",
        "6. **Normalization of Weights**: The sample weights are then normalized to ensure they sum up to 1. This step prevents the weights from becoming too large over iterations.\n",
        "\n",
        "7. **Aggregate Predictions**: The weak learner’s prediction is combined with the predictions from previous weak learners, each weighted by its corresponding alpha value. This creates the ensemble prediction.\n",
        "\n",
        "8. **Repeat**: Steps 2 to 7 are repeated for a specified number of iterations or until a certain level of accuracy is achieved.\n",
        "\n",
        "9. **Final Prediction**: The final prediction is made by **combining the weighted predictions of all the weak learners in the ensemble**. The **alpha values also contribute to each weak learner’s prediction weight**."
      ],
      "metadata": {
        "id": "LRmCL_siHrmV"
      },
      "id": "LRmCL_siHrmV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Sequential Learning\n",
        "- Focus on Difficult samples\n",
        "- Weak Learners becomes strong"
      ],
      "metadata": {
        "id": "-30FXUbdOyUR"
      },
      "id": "-30FXUbdOyUR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea behind AdaBoost is that, by sequentially focusing on the samples misclassified by previous weak learners, the algorithm adapts to the characteristics of the data and improves its overall predictive power. The final ensemble prediction is usually a weighted majority vote or a weighted sum of the individual weak learners’ predictions."
      ],
      "metadata": {
        "id": "aaOS_VvqHros"
      },
      "id": "aaOS_VvqHros"
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost **starts by training a weak learner (or stump) and adds more learners until there is an ensemble of weak learners.**\n",
        "\n",
        "AdaBoost’s strength lies in **its ability to turn a collection of weak learners into a strong ensemble learner, often achieving impressive predictive performance**. However, it’s essential to be cautious of **overfitting**, especially if the weak learners are too complex. Also, AdaBoost may struggle with **noisy data or outliers** that repeatedly get misclassified.\n",
        "\n"
      ],
      "metadata": {
        "id": "DNaJHwvaHrrd"
      },
      "id": "DNaJHwvaHrrd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages"
      ],
      "metadata": {
        "id": "zq9XyaLTHruF"
      },
      "id": "zq9XyaLTHruF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **High Accuracy**: It often achieves higher accuracy than a single model. It combines the strengths of multiple weak learners to create a strong ensemble that can generalize well to unseen data.\n",
        "\n",
        "- **Flexibility**: AdaBoost can work with various base learners (weak learners), such as decision stumps, linear models, or even more complex models. This flexibility allows it to adapt to different data types and problem domains.\n",
        "\n",
        "- **Feature Importance**: It provides a way to estimate feature importance by observing how often a feature is used in the ensemble of weak learners. This information can be helpful for feature selection or understanding the importance of different input variables.\n",
        "\n",
        "- **Handles Noisy Data**: AdaBoost is less prone to overfitting, even when dealing with noisy data. By focusing on the misclassified samples in each iteration, the algorithm adapts to the data’s noise and prevents individual weak learners from fitting to the noise.\n",
        "\n",
        "- **Automatic Feature Scaling**: It doesn’t require manual feature scaling, as it uses sample weights to emphasize misclassified samples, effectively achieving a form of automatic feature scaling.\n",
        "\n",
        "- **No Need for Complex Tuning**: While some hyperparameter tuning is necessary, AdaBoost performs well with a reasonable choice of hyperparameters, making it relatively easy to use."
      ],
      "metadata": {
        "id": "cHyxJO4nJHvk"
      },
      "id": "cHyxJO4nJHvk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disadvantages"
      ],
      "metadata": {
        "id": "k1ZacTN0JjxF"
      },
      "id": "k1ZacTN0JjxF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Sensitive to Noisy Data and Outliers**: While AdaBoost is generally robust to noisy data, it can be sensitive to outliers, especially when using complex base learners. Outliers can lead to overemphasis on these data points during training.\n",
        "\n",
        "- **Overfitting with Complex Models**: If the base learners are too complex, it can still suffer from overfitting, mainly if the number of iterations (n_estimators) is too high. Careful selection of base learners and hyperparameters is essential.\n",
        "\n",
        "- **Computationally Intensive**: Training multiple weak learners sequentially and adjusting sample weights can be computationally intensive, especially if the dataset is large or the base learners are complex.\n",
        "\n",
        "- **Bias towards Uniform Distributions**: AdaBoost works best when the dataset is well-balanced, and the class distribution is roughly uniform. In cases where one class significantly outweighs the others, AdaBoost might be biased toward the majority class.\n",
        "\n",
        "- **Prone to Model Instability**: AdaBoost can perform poorly if the weak learners are too complex, leading to model instability. Weak learners should be slightly better than random guessing to achieve optimal results.\n",
        "\n",
        "- **Hyperparameter Tuning**: While AdaBoost is relatively simple, tuning the number of iterations (n_estimators) and other hyperparameters can be time-consuming and require cross-validation."
      ],
      "metadata": {
        "id": "8-R_ccg_JlzT"
      },
      "id": "8-R_ccg_JlzT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost hyperparameter tuning"
      ],
      "metadata": {
        "id": "SWS7gWWFKLJl"
      },
      "id": "SWS7gWWFKLJl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning is an essential step in optimizing the performance of machine learning algorithms, including AdaBoost. Selecting the correct hyperparameters can achieve better generalization and more accurate predictions. Here are some key hyperparameters to consider when tuning an AdaBoost model:\n",
        "\n"
      ],
      "metadata": {
        "id": "wurEBgPCKMxj"
      },
      "id": "wurEBgPCKMxj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Number of Estimators (n_estimators)**: This hyperparameter determines the number of weak learners (base estimators) trained and combined in the ensemble. Increasing the number of estimators can improve performance but also increases computation time. However, there’s a point **where adding more estimators might lead to overfitting. Cross-validation can help determine an optimal value.**\n",
        "\n",
        "- **Learning Rate (learning_rate)**: The learning rate controls the **contribution of each weak learner to the final ensemble**. Smaller learning rates make the training process slower but can help prevent overfitting. Larger learning rates can speed up training but might make the model prone to overfitting. It’s often used in conjunction with n_estimators.\n",
        "\n",
        "- **Base Estimator**: The choice of the base estimator (weak learner) can impact the model’s performance. The **base estimator should be simple and perform slightly better than random guessing**. Common choices include decision stumps (shallow decision trees with one level) or linear models. You can also use different types of base estimators and see which works best for your data.\n",
        "\n",
        "- **Base Estimator Hyperparameters**: If your base estimator has hyperparameters (e.g., max_depth for decision trees), tuning them can influence the performance of the AdaBoost model.\n",
        "\n",
        "- **Loss Function (loss)**: AdaBoost supports different loss functions for classification, such as **exponential, linear, and square**. The choice of the loss function can impact how the model assigns weights to misclassified samples.\n",
        "\n",
        "- **Random Seed (random_state)**: Setting the random seed ensures reproducibility. However, when performing hyperparameter tuning, you might want to try different random seeds to ensure that the chosen hyperparameters are robust across different random initializations.\n",
        "\n",
        "- **Feature Sampling (max_features)**: For decision tree-based base estimators, you can control the **maximum number of features considered for splitting at each node**. This can help in reducing overfitting and speeding up training.\n",
        "\n",
        "- **Cross-Validation**: Cross-validation is crucial for hyperparameter tuning. It helps estimate **the model’s performance on unseen data and prevents overfitting the training set**. You can use techniques like grid search or random search to explore the hyperparameter space."
      ],
      "metadata": {
        "id": "MZ7dz2E1KOT8"
      },
      "id": "MZ7dz2E1KOT8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# variations of AdaBoost"
      ],
      "metadata": {
        "id": "-TqQF6maM3dr"
      },
      "id": "-TqQF6maM3dr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several variations and extensions of the classic AdaBoost algorithm are designed to address specific limitations or improve performance in different scenarios. Some of the notable AdaBoost variations include:"
      ],
      "metadata": {
        "id": "9lb4J4tiM6Np"
      },
      "id": "9lb4J4tiM6Np"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss)** is a variant designed for **multi-class classification problems**. It extends AdaBoost’s binary classification approach to handle multiple classes. SAMME assigns weights to each class and adapts these weights during each iteration to create an ensemble of weak learners that collectively classify multiple classes.\n",
        "\n",
        "- **SAMME.R** is an extension of SAMME that works with **real-valued predictions**. Instead of using discrete class labels, SAMME.R takes the predicted class probabilities from the base learners. This variant can improve performance in scenarios where class probabilities provide more information than class labels.\n",
        "\n",
        "- **AdaBoost.R2** is an adaptation of **AdaBoost for regression problems**. Unlike traditional version, which focuses on minimizing classification errors, AdaBoost.R2 minimizes the loss function associated with regression tasks, such as mean squared error.\n",
        "\n",
        "- **Real AdaBoost** is an alternative version of AdaBoost that modifies the **weight update formula to correct some of the weaknesses of the original version**. It addresses the issue of overemphasis on misclassified samples by using a modified weight update equation.\n",
        "\n",
        "- **Adaptive Resampling AdaBoost (AR-AdaBoost)** introduces adaptive resampling during training. Each iteration selects the next weak learner based on the misclassified samples’ distribution. This can lead to improved performance, especially when dealing with imbalanced datasets.\n",
        "\n",
        "- **Cost-Sensitive AdaBoost** introduces costs associated with misclassifying different classes. It modifies the weight update equation to consider these costs, which is advantageous when classifying instances of different classes associated with varying degrees of cost.\n",
        "\n",
        "- **AdaBoost.MH (AdaBoost with Margin Heuristic**) introduces a margin heuristic that considers the confidence of each weak learner’s prediction. It aims to increase the margins between the correct and incorrect predictions, potentially improving generalization.\n",
        "\n",
        "- **Kernelized AdaBoost** extends AdaBoost to work with non-linearly separable data by introducing kernel functions to map the data into a higher-dimensional space. This enables AdaBoost to handle more complex decision boundaries.\n",
        "\n",
        "- **LP-AdaBoost (Label Propagation AdaBoost)** combines AdaBoost with label propagation techniques to improve performance in semi-supervised and transductive learning scenarios. It incorporates information from unlabeled instances to enhance the learning process."
      ],
      "metadata": {
        "id": "bZhwxzx8M6ei"
      },
      "id": "bZhwxzx8M6ei"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications of AdaBoost"
      ],
      "metadata": {
        "id": "T5iz3_GqPBQ5"
      },
      "id": "T5iz3_GqPBQ5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "AdaBoost has been successfully applied in numerous domains, including:\n",
        "\n",
        "* **Face Detection:** The Viola-Jones algorithm, a pioneering work in real-time object detection (most notably for faces), famously uses AdaBoost to combine simple Haar-like features into a robust cascade classifier.\n",
        "* **Object Recognition:** Identifying and categorizing various objects within images or video streams.\n",
        "* **Text Classification and Sentiment Analysis:** Classifying documents or predicting the sentiment of text data (e.g., positive, negative, neutral).\n",
        "* **Medical Diagnosis:** Assisting in the diagnosis of diseases by identifying patterns in patient data.\n",
        "* **Cybersecurity:** Used in intrusion detection systems and anomaly detection to flag suspicious activities.\n",
        "* **Fraud Detection:** Identifying fraudulent transactions in financial data.\n",
        "* **Bioinformatics:** Various applications, including gene expression analysis."
      ],
      "metadata": {
        "id": "LmZRDDjRO9h6"
      },
      "id": "LmZRDDjRO9h6"
    },
    {
      "cell_type": "markdown",
      "id": "85c64875",
      "metadata": {
        "id": "85c64875"
      },
      "source": [
        "# Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8366b7",
      "metadata": {
        "id": "6b8366b7"
      },
      "source": [
        "- https://spotintelligence.com/2023/08/21/adaboost-practical-introduction-with-how-to-python-tutorial-for-classification-regression/"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}